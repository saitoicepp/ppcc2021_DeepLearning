{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 畳み込みニューラルネットワーク (CNN) \n",
    "CNN は画像認識の分野で非常によく使われています。\n",
    "ここでは CNNをPyTorchで実装して、手書き文字認識(MNIST)の問題を解いてみます。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorchが使うCPUの数を制限します。(VMを使う場合)\n",
    "%env OMP_NUM_THREADS=1\n",
    "%env MKL_NUM_THREADS=1\n",
    "\n",
    "from torch import set_num_threads, set_num_interop_threads\n",
    "num_threads = 1\n",
    "set_num_threads(num_threads)\n",
    "set_num_interop_threads(num_threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#ライブラリのインポート\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchinfo\n",
    "from tqdm import tqdm\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST データセットのインポート\n",
    "\n",
    "MNISTデータセットを`/data/staff/deeplearning/datasets_pytorch`にダウンロードしてあります。以下のようにしてここからデータをロードできます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST データセットのインポート\n",
    "import torchvision\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    \"/data/staff/deeplearning/datasets_pytorch\",\n",
    "    train=True,\n",
    "    transform=torchvision.transforms.ToTensor(),\n",
    ")\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    \"/data/staff/deeplearning/datasets_pytorch\",\n",
    "    train=False,\n",
    "    transform=torchvision.transforms.ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "もし他の場所にダウンロードしたい方は以下のようにしてください。\n",
    "```python\n",
    "from torchvision import datasets, transforms\n",
    "train_dataset = datasets.MNIST(YOUR_FAVORITE_PATH, train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = datasets.MNIST(YOUR_FAVORITE_PATH, train=False, transform=transforms.ToTensor(), download=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`train_dataset`は学習用データセット、`test_dataset`はモデル評価用データセットです。\n",
    "それぞれのデータセットのエントリーは以下のようにして確認できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_0th, label_0th = train_dataset[0]\n",
    "print('image shape = ', image_0th.shape)\n",
    "print('label = ', label_0th)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ミニバッチの実装を簡単にするため、DataLoaderを使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=100, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoaderを使うと、以下のように指定したバッチサイズでデータが簡単に取得できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, labels in train_dataloader:\n",
    "    # 指定したバッチサイズ(ここでは100)ごとにデータが切り出されます。\n",
    "    print(f\"image shape = {images.shape}, labels shape = {labels.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNISTのトレーニング用データは6万画像ありますが、このノートブック内では計算時間を短くするため、6000画像だけを使うことにします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "\n",
    "train_dataloader = DataLoader(Subset(train_dataset, np.arange(6000)), batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST 画像の表示\n",
    "画像と、それに対応するラベルを見てみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figureの作成 (キャンバスの作成)\n",
    "fig, ax = plt.subplots(1, 3, figsize=(12, 6))\n",
    "\n",
    "for idx in range(3):\n",
    "    # 各画像をプロット\n",
    "    ax[idx].imshow(train_dataset[idx][0].squeeze())\n",
    "print(\"label = \", [train_dataset[idx][1] for idx in range(3)])\n",
    "\n",
    "# 図を表示\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN モデルの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(3, 3), stride=(1, 1), padding=\"valid\"),\n",
    "    nn.MaxPool2d(kernel_size=(2, 2)),\n",
    "    nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(3, 3), stride=(1, 1), padding=\"valid\"),\n",
    "    nn.MaxPool2d(kernel_size=(2, 2)),\n",
    "    nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(3, 3), stride=(1, 1), padding=\"valid\"),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(in_features=576, out_features=64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=64, out_features=10),\n",
    "    # nn.Softmax(dim=1)  # PyTorchではロス関数に CrossEntropyLoss を指定すると、自動でSoftmaxが適用されます。そのため、モデルにSoftmaxを適用する必要はありません。\n",
    ")\n",
    "\n",
    "torchinfo.summary(\n",
    "    model, input_size=next(iter(train_dataloader))[0].shape, col_names=[\"output_size\", \"num_params\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### トレーニング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# トレーニング\n",
    "num_epochs = 10\n",
    "for i_epoch in range(num_epochs):\n",
    "\n",
    "    # エポックごとのロス、accuracyを計算するための変数\n",
    "    loss_total = 0.0\n",
    "    accuracy_total = 0.0\n",
    "\n",
    "    for images, labels in train_dataloader:\n",
    "        # モデルをトレーニングモードにする。\n",
    "        model.train()\n",
    "\n",
    "        # 順伝搬\n",
    "        y_pred = model(images)\n",
    "\n",
    "        # ロスの計算\n",
    "        loss = loss_fn(y_pred, labels)\n",
    "\n",
    "        # 誤差逆伝播の前に各パラメータの勾配の値を0にセットする。\n",
    "        # これをしないと、勾配の値はそれまでの値との和がとられる。\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 誤差逆伝播。各パラメータの勾配が計算される。\n",
    "        loss.backward()\n",
    "\n",
    "        # 各パラメータの勾配の値を基に、optimizerにより値が更新される。\n",
    "        optimizer.step()\n",
    "\n",
    "        # ミニバッチごとのロス、accuracyを記録\n",
    "        loss_total += loss.detach().numpy()\n",
    "\n",
    "        # 正解率(Accuracy)の計算\n",
    "        label_pred = y_pred.max(dim=1)[1]  # 予測ラベル\n",
    "        num_correct = (label_pred == labels).sum().numpy()  # 正解数\n",
    "        accuracy_total += num_correct / len(images)  # 平均正解数\n",
    "\n",
    "    # ロス、accuracyをミニバッチの数で割って平均を取ります。\n",
    "    loss_total /= len(train_dataloader)\n",
    "    accuracy_total /= len(train_dataloader)\n",
    "    print(f\"epoch = {i_epoch}, loss = {loss_total}, acc = {accuracy_total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 性能評価\n",
    "性能評価用のデータセット(`test_dataloader`)を使って性能評価してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの性能評価\n",
    "loss_total = 0\n",
    "accuracy_total = 0.0\n",
    "\n",
    "# モデルを評価モードにする。\n",
    "model.eval()\n",
    "\n",
    "for images, labels in test_dataloader:\n",
    "    # 順伝搬\n",
    "    y_pred = model(images)\n",
    "\n",
    "    # ロスの計算\n",
    "    loss = loss_fn(y_pred, labels)\n",
    "\n",
    "    # ミニバッチごとのロス、accuracyを記録\n",
    "    loss_total += loss.detach().numpy()\n",
    "\n",
    "    # 正解率(Accuracy)の計算\n",
    "    label_pred = y_pred.max(dim=1)[1]  # 予測ラベル\n",
    "    num_correct = (label_pred == labels).sum().numpy()  # 正解数\n",
    "    accuracy_total += num_correct / len(images)  # 平均正解数\n",
    "\n",
    "loss_total /= len(test_dataloader)\n",
    "accuracy_total /= len(test_dataloader)\n",
    "print(f\"loss = {loss_total}, acc = {accuracy_total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "accuracy が 95%以上と、良い精度で判別ができていると思います。\n",
    "間違った画像がどのようなものかも確認してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルを評価モードにする。\n",
    "model.eval()\n",
    "\n",
    "# Figureの作成 (キャンバスの作成)\n",
    "fig, ax = plt.subplots(1, 3, figsize=(12, 6))\n",
    "\n",
    "for images, labels in test_dataloader:\n",
    "    y_pred = model(images)\n",
    "    label_pred = y_pred.max(dim=1)[1]  # 予測ラベル\n",
    "\n",
    "    # 予測値と正解ラベルが一致していないエントリーを抽出します。\n",
    "    wrong_image_indices = (label_pred != labels).nonzero().numpy()\n",
    "\n",
    "    for i, idx in enumerate(wrong_image_indices):\n",
    "        ax[i].imshow(images[idx].squeeze())\n",
    "        print(f\"label = {labels[idx].squeeze().numpy()}\")\n",
    "        print(f\"prediction = {label_pred[idx].squeeze().numpy()}\")\n",
    "\n",
    "    break\n",
    "\n",
    "# 図を表示\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (おまけ) CNNとMLPの比較\n",
    "MNIST を MLPで解くとどうなるかも調べてみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの定義\n",
    "model_dnn = nn.Sequential(\n",
    "    nn.Flatten(),  # 画像を1次元のベクトルに変換: 28 * 28 = 784\n",
    "    nn.Linear(in_features=784, out_features=128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=128, out_features=128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=128, out_features=128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=128, out_features=128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=128, out_features=128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=128, out_features=10),\n",
    ")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_dnn.parameters())\n",
    "\n",
    "# トレーニング\n",
    "num_epochs = 10\n",
    "for i_epoch in range(num_epochs):\n",
    "\n",
    "    loss_total = 0.0\n",
    "    accuracy_total = 0.0\n",
    "\n",
    "    for images, labels in train_dataloader:\n",
    "        # モデルをトレーニングモードにする。\n",
    "        model_dnn.train()\n",
    "\n",
    "        # 順伝搬\n",
    "        y_pred = model_dnn(images)\n",
    "\n",
    "        # ロスの計算\n",
    "        loss = loss_fn(y_pred, labels)\n",
    "\n",
    "        # 誤差逆伝播の前に各パラメータの勾配の値を0にセットする。\n",
    "        # これをしないと、勾配の値はそれまでの値との和がとられる。\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 誤差逆伝播。各パラメータの勾配が計算される。\n",
    "        loss.backward()\n",
    "\n",
    "        # 各パラメータの勾配の値を基に、optimizerにより値が更新される。\n",
    "        optimizer.step()\n",
    "\n",
    "        # ミニバッチごとのロス、accuracyを記録\n",
    "        loss_total += loss.detach().numpy()\n",
    "\n",
    "        # 正解率(Accuracy)の計算\n",
    "        label_pred = y_pred.max(dim=1)[1]  # 予測ラベル\n",
    "        num_correct = (label_pred == labels).sum().numpy()  # 正解数\n",
    "        accuracy_total += num_correct / len(images)  # 平均正解数\n",
    "\n",
    "    # ロス、accuracyをミニバッチの数で割って平均を取ります。\n",
    "    loss_total /= len(train_dataloader)\n",
    "    accuracy_total /= len(train_dataloader)\n",
    "    print(f\"epoch = {i_epoch}, loss = {loss_total}, acc = {accuracy_total}\")\n",
    "\n",
    "# モデルの性能評価\n",
    "loss_total = 0\n",
    "accuracy_total = 0.0\n",
    "\n",
    "# モデルを評価モードにする。\n",
    "model_dnn.eval()\n",
    "for images, labels in test_dataloader:\n",
    "    # 順伝搬\n",
    "    y_pred = model_dnn(images)\n",
    "\n",
    "    # ロスの計算\n",
    "    loss = loss_fn(y_pred, labels)\n",
    "\n",
    "    # ミニバッチごとのロス、accuracyを記録\n",
    "    loss_total += loss.detach().numpy()\n",
    "\n",
    "    # 正解率(Accuracy)の計算\n",
    "    label_pred = y_pred.max(dim=1)[1]  # 予測ラベル\n",
    "    num_correct = (label_pred == labels).sum().numpy()  # 正解数\n",
    "    accuracy_total += num_correct / len(images)  # 平均正解数\n",
    "\n",
    "loss_total /= len(test_dataloader)\n",
    "accuracy_total /= len(test_dataloader)\n",
    "print(f\"test loss = {loss_total}, test acc = {accuracy_total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "どのくらいの精度が出たでしょうか？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次は、画像のピクセルのシャッフルをしてみます。\n",
    "\n",
    "これを画像としてプロットすると、人間には理解不能なものになっていることがわかります。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = train_dataset[0][0]\n",
    "\n",
    "# 全ての画像に対して、同じルールでピクセルのシャッフルをします。\n",
    "permute = np.random.permutation(28 * 28)\n",
    "image = image.numpy().flatten()[permute].reshape([1, 28, 28])\n",
    "\n",
    "plt.imshow(image.squeeze())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これをCNN, MLPで学習させると、どうなるでしょうか？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN の学習\n",
    "\n",
    "model_cnn = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(3, 3)),\n",
    "    nn.MaxPool2d(kernel_size=(2, 2)),\n",
    "    nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(3, 3)),\n",
    "    nn.MaxPool2d(kernel_size=(2, 2)),\n",
    "    nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(3, 3)),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(in_features=576, out_features=64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=64, out_features=10),\n",
    ")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_cnn.parameters())\n",
    "\n",
    "# トレーニング\n",
    "num_epochs = 10\n",
    "for i_epoch in range(num_epochs):\n",
    "\n",
    "    loss_total = 0.0\n",
    "    accuracy_total = 0.0\n",
    "\n",
    "    for images, labels in train_dataloader:\n",
    "        # モデルをトレーニングモードにする。\n",
    "        model_cnn.train()\n",
    "\n",
    "        # 全ての画像に対して、同じルールでピクセルのシャッフルをします。\n",
    "        images = images.flatten(start_dim=2)[:, :, permute].reshape([-1, 1, 28, 28])\n",
    "\n",
    "        # 順伝搬\n",
    "        y_pred = model_cnn(images)\n",
    "\n",
    "        # ロスの計算\n",
    "        loss = loss_fn(y_pred, labels)\n",
    "\n",
    "        # 誤差逆伝播の前に各パラメータの勾配の値を0にセットする。\n",
    "        # これをしないと、勾配の値はそれまでの値との和がとられる。\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 誤差逆伝播。各パラメータの勾配が計算される。\n",
    "        loss.backward()\n",
    "\n",
    "        # 各パラメータの勾配の値を基に、optimizerにより値が更新される。\n",
    "        optimizer.step()\n",
    "\n",
    "        # ミニバッチごとのロス、accuracyを記録\n",
    "        loss_total += loss.detach().numpy()\n",
    "\n",
    "        # 正解率(Accuracy)の計算\n",
    "        label_pred = y_pred.max(dim=1)[1]  # 予測ラベル\n",
    "        num_correct = (label_pred == labels).sum().numpy()  # 正解数\n",
    "        accuracy_total += num_correct / len(images)  # 平均正解数\n",
    "\n",
    "    # ロス、accuracyをミニバッチの数で割って平均を取ります。\n",
    "    loss_total /= len(train_dataloader)\n",
    "    accuracy_total /= len(train_dataloader)\n",
    "    print(f\"epoch = {i_epoch}, loss = {loss_total}, acc = {accuracy_total}\")\n",
    "\n",
    "\n",
    "# モデルの性能評価\n",
    "loss_total = 0\n",
    "accuracy_total = 0.0\n",
    "\n",
    "# モデルを評価モードにする。\n",
    "model_cnn.eval()\n",
    "for images, labels in test_dataloader:\n",
    "    # 全ての画像に対して、同じルールでピクセルのシャッフルをします。\n",
    "    images = images.flatten(start_dim=2)[:, :, permute].reshape([-1, 1, 28, 28])\n",
    "\n",
    "    # 順伝搬\n",
    "    y_pred = model_cnn(images)\n",
    "\n",
    "    # ロスの計算\n",
    "    loss = loss_fn(y_pred, labels)\n",
    "\n",
    "    # ミニバッチごとのロス、accuracyを記録\n",
    "    loss_total += loss.detach().numpy()\n",
    "\n",
    "    # 正解率(Accuracy)の計算\n",
    "    label_pred = y_pred.max(dim=1)[1]  # 予測ラベル\n",
    "    num_correct = (label_pred == labels).sum().numpy()  # 正解数\n",
    "    accuracy_total += num_correct / len(images)  # 平均正解数\n",
    "\n",
    "loss_total /= len(test_dataloader)\n",
    "accuracy_total /= len(test_dataloader)\n",
    "print(f\"test loss = {loss_total}, test acc = {accuracy_total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの定義\n",
    "model_dnn = nn.Sequential(\n",
    "    nn.Flatten(),  # 画像を1次元のベクトルに変換: 28 * 28 = 784\n",
    "    nn.Linear(in_features=784, out_features=128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=128, out_features=128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=128, out_features=128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=128, out_features=128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=128, out_features=128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=128, out_features=10),\n",
    ")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_dnn.parameters())\n",
    "\n",
    "# トレーニング\n",
    "num_epochs = 10\n",
    "for i_epoch in range(num_epochs):\n",
    "\n",
    "    loss_total = 0.0\n",
    "    accuracy_total = 0.0\n",
    "\n",
    "    for images, labels in train_dataloader:\n",
    "        # モデルをトレーニングモードにする。\n",
    "        model_dnn.train()\n",
    "\n",
    "        # 全ての画像に対して、同じルールでピクセルのシャッフルをします。\n",
    "        images = images.flatten(start_dim=2)[:, :, permute].reshape([-1, 1, 28, 28])\n",
    "\n",
    "        # 順伝搬\n",
    "        y_pred = model_dnn(images)\n",
    "\n",
    "        # ロスの計算\n",
    "        loss = loss_fn(y_pred, labels)\n",
    "\n",
    "        # 誤差逆伝播の前に各パラメータの勾配の値を0にセットする。\n",
    "        # これをしないと、勾配の値はそれまでの値との和がとられる。\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 誤差逆伝播。各パラメータの勾配が計算される。\n",
    "        loss.backward()\n",
    "\n",
    "        # 各パラメータの勾配の値を基に、optimizerにより値が更新される。\n",
    "        optimizer.step()\n",
    "\n",
    "        # ミニバッチごとのロス、accuracyを記録\n",
    "        loss_total += loss.detach().numpy()\n",
    "\n",
    "        # 正解率(Accuracy)の計算\n",
    "        label_pred = y_pred.max(dim=1)[1]  # 予測ラベル\n",
    "        num_correct = (label_pred == labels).sum().numpy()  # 正解数\n",
    "        accuracy_total += num_correct / len(images)  # 平均正解数\n",
    "\n",
    "    # ロス、accuracyをミニバッチの数で割って平均を取ります。\n",
    "    loss_total /= len(train_dataloader)\n",
    "    accuracy_total /= len(train_dataloader)\n",
    "    print(f\"epoch = {i_epoch}, loss = {loss_total}, acc = {accuracy_total}\")\n",
    "\n",
    "# モデルの性能評価\n",
    "loss_total = 0\n",
    "accuracy_total = 0.0\n",
    "\n",
    "# モデルを評価モードにする。\n",
    "model_dnn.eval()\n",
    "for images, labels in test_dataloader:\n",
    "    # 全ての画像に対して、同じルールでピクセルのシャッフルをします。\n",
    "    images = images.flatten(start_dim=2)[:, :, permute].reshape([-1, 1, 28, 28])\n",
    "\n",
    "    # 順伝搬\n",
    "    y_pred = model_dnn(images)\n",
    "\n",
    "     # ロスの計算\n",
    "    loss = loss_fn(y_pred, labels)\n",
    "\n",
    "    # ミニバッチごとのロス、accuracyを記録\n",
    "    loss_total += loss.detach().numpy()\n",
    "\n",
    "    # 正解率(Accuracy)の計算\n",
    "    label_pred = y_pred.max(dim=1)[1]  # 予測ラベル\n",
    "    num_correct = (label_pred == labels).sum().numpy()  # 正解数\n",
    "    accuracy_total += num_correct / len(images)  # 平均正解数\n",
    "\n",
    "loss_total /= len(test_dataloader)\n",
    "accuracy_total /= len(test_dataloader)\n",
    "print(f\"test loss = {loss_total}, test acc = {accuracy_total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "画像のピクセルをシャッフルする前と比べて、CNN/DNNの性能はどのように変化したでしょうか？"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
