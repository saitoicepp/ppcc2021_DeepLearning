{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ライブラリのインポート\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPUによる高速化\n",
    "\n",
    "TensorflowやPyTorchといった深層学習ライブラリはGPUでの実行がサポートされています。\n",
    "GPUを使うことで大規模計算が高速化できることがあります。\n",
    "\n",
    "このノートブックではTensorflow/KerasやPyTorchをGPU上で実行し、CPUでの実行したときと計算速度を比較します。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPUの利用状況の確認\n",
    "\n",
    "現在接続しているノードのGPU使用率は以下のコマンドで確認できます。\n",
    "```bash\n",
    "$ nvidia-smi\n",
    "Thu Jul 20 11:29:18 2023       \n",
    "+-----------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 520.61.05    Driver Version: 520.61.05    CUDA Version: 11.8     |\n",
    "|-------------------------------+----------------------+----------------------+\n",
    "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                               |                      |               MIG M. |\n",
    "|===============================+======================+======================|\n",
    "|   0  NVIDIA A100-PCI...  On   | 00000000:1F:00.0 Off |                    0 |\n",
    "| N/A   84C    P0   129W / 250W |   3493MiB / 40960MiB |     78%      Default |\n",
    "|                               |                      |             Disabled |\n",
    "+-------------------------------+----------------------+----------------------+\n",
    "|   1  NVIDIA A100-PCI...  On   | 00000000:20:00.0 Off |                    0 |\n",
    "| N/A   84C    P0   175W / 250W |   2823MiB / 40960MiB |     79%      Default |\n",
    "|                               |                      |             Disabled |\n",
    "+-------------------------------+----------------------+----------------------+\n",
    "\n",
    "+-----------------------------------------------------------------------------+\n",
    "| Processes:                                                                  |\n",
    "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
    "|        ID   ID                                                   Usage      |\n",
    "|=============================================================================|\n",
    "|    0   N/A  N/A   3460006      C   python                           1242MiB |\n",
    "|    0   N/A  N/A   3461170      C   ...HostWorker.run_executable     1494MiB |\n",
    "|    1   N/A  N/A   3460006      C   python                            598MiB |\n",
    "|    1   N/A  N/A   3461170      C   ...HostWorker.run_executable     2216MiB |\n",
    "+-----------------------------------------------------------------------------+\n",
    "```\n",
    "このノードにはGPUが2台搭載されており、それぞれ2つのプロセスがGPUを使用しています。\n",
    "\n",
    "自分のプロセスがゾンビ状態となってGPUのメモリを専有してしまうことがないように定期的にチェックするのが好ましいです。\n",
    "\n",
    "GPUを使っているプロセスを起動したユーザーを確認するコマンドの一例は以下です。\n",
    "```bash\n",
    "$ ps -up `nvidia-smi --query-compute-apps=pid --format=csv,noheader | sort -u`\n",
    "USER         PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\n",
    "```\n",
    "ここで表示されるメモリの値はGPU上のメモリではないことに注意してください。\n",
    "\n",
    "notebook上では\"!\"をつけることでbashコマンドが実行できます。     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ps -up `nvidia-smi --query-compute-apps=pid --format=csv,noheader | sort -u`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用するGPUの指定\n",
    "まずは使うGPUを指定します。このステップなしでもGPUは利用できますが、GPUを専有してしまうことで共有マシンを使用している他のユーザーに迷惑をかけてしまうことがあります。\n",
    "今回の講習では、使うGPUは1つのみに限定させます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_available_gpu_index():\n",
    "    import random\n",
    "    import subprocess\n",
    "\n",
    "    # ノード上のGPUのインデックスとuuidを取得します。\n",
    "    p = subprocess.run(\n",
    "        f\"nvidia-smi --query-gpu=index,gpu_uuid --format=csv,noheader\",\n",
    "        shell=True,\n",
    "        stdout=subprocess.PIPE,\n",
    "    )\n",
    "    gpu_uuid_to_index = {}\n",
    "    for v in p.stdout.decode().split('\\n'):\n",
    "        if len(v) == 0:\n",
    "            continue\n",
    "        index, uuid = v.split(',')\n",
    "        gpu_uuid_to_index[uuid.strip()] = index.strip()\n",
    "\n",
    "    # GPUを使っているプロセスのuuidを取得します。\n",
    "    p = subprocess.run(\n",
    "        f\"nvidia-smi --query-compute-apps=gpu_uuid --format=csv,noheader\",\n",
    "        shell=True,\n",
    "        stdout=subprocess.PIPE,\n",
    "    )\n",
    "    ret = p.stdout.decode().split('\\n')\n",
    "    ret = set(ret)\n",
    "    for uuid in ret:\n",
    "        if len(uuid) == 0:\n",
    "            continue\n",
    "        gpu_uuid_to_index.pop(uuid)\n",
    "\n",
    "    # どのプロセスからも使用されていないGPUのインデックスをランダムに一つ返します。\n",
    "    # GPUが使用できない場合は空文字列を返します。\n",
    "    if len(gpu_uuid_to_index) == 0:\n",
    "        print('GPU is unavailable.')\n",
    "        return \"\"\n",
    "    else:\n",
    "        available_gpu_index = list(gpu_uuid_to_index.values())\n",
    "        return random.choice(available_gpu_index)\n",
    "\n",
    "# 使用されていないGPUを選択します。\n",
    "gpu_index = get_available_gpu_index()\n",
    "print(gpu_index)\n",
    "\n",
    "# 使うGPUを一つに制限します。\n",
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = gpu_index\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow でのGPU利用の注意点\n",
    "\n",
    "TensorflowはデフォルトでGPU上のメモリを全て確保しようとします。GPUを利用するのが1人だけの場合はこれで良いのですが、複数人で共有する場合は他のユーザーのプロセスを停止させてしまうこともあります。\n",
    "そのため、Tensorflowが必要な分だけGPUメモリを確保するように設定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action=\"ignore\", category=DeprecationWarning)\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "# Tensorflowが使うCPUの数を制限します。(VMを使う場合)\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "os.environ['TF_NUM_INTEROP_THREADS'] = '1'\n",
    "os.environ['TF_NUM_INTRAOP_THREADS'] = '1'\n",
    "\n",
    "from tensorflow.config import threading\n",
    "num_threads = 1\n",
    "threading.set_inter_op_parallelism_threads(num_threads)\n",
    "threading.set_intra_op_parallelism_threads(num_threads)\n",
    "\n",
    "# GPUのメモリを使いすぎないように制限します。\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "    tf.config.set_visible_devices(physical_devices, 'GPU')\n",
    "    for gpu in physical_devices:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print('available GPU:', logical_gpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLPのトレーニング\n",
    "\n",
    "MLPのトレーニングをCPU/GPUでそれぞれ実行することで、計算時間の変化を確認します。\n",
    "\n",
    "データセットは乱数で適当に作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import default_rng\n",
    "rng = default_rng(seed=0)\n",
    "\n",
    "# 10万行、入力次元10のランダムなデータを作成。\n",
    "datasize = 100000\n",
    "x = rng.normal(size=(datasize, 10))\n",
    "t = rng.integers(0, 2, size=(datasize, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPUによるサンプルコード"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPUがあるとGPUが自動で使われてしまいます。CPUで計算するように明示的に指定することでCPUで計算させることができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "with tf.device('/CPU:0'):\n",
    "    # モデルの定義\n",
    "    model = Sequential([\n",
    "        Dense(units=256, activation='relu', input_dim=10),  # ノード数が256の層を追加。活性化関数はReLU。\n",
    "        Dense(units=256, activation='relu'),  # ノード数が256の層を追加。活性化関数はReLU。\n",
    "        Dense(units=256, activation='relu'),  # ノード数が256の層を追加。活性化関数はReLU。\n",
    "        Dense(units=256, activation='relu'),  # ノード数が256の層を追加。活性化関数はReLU。\n",
    "        Dense(units=256, activation='relu'),  # ノード数が256の層を追加。活性化関数はReLU。\n",
    "        Dense(units=1, activation='sigmoid')  # ノード数が1の層を追加。活性化関数はシグモイド関数。\n",
    "    ])\n",
    "\n",
    "    #  誤差関数としてクロスエントロピーを指定。最適化手法はadam\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "    #  トレーニング\n",
    "    model.fit(\n",
    "        x=x,\n",
    "        y=t,\n",
    "        batch_size=1024,  # バッチサイズ。一回のステップで1024行のデータを使うようにする。\n",
    "        epochs=10,  # 学習のステップ数\n",
    "        verbose=1,  # 1とするとステップ毎に誤差関数の値などが表示される\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPUによるサンプルコード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# モデルの定義\n",
    "model = Sequential([\n",
    "    Dense(units=256, activation='relu', input_dim=10),  # ノード数が256の層を追加。活性化関数はReLU。\n",
    "    Dense(units=256, activation='relu'),  # ノード数が256の層を追加。活性化関数はReLU。\n",
    "    Dense(units=256, activation='relu'),  # ノード数が256の層を追加。活性化関数はReLU。\n",
    "    Dense(units=256, activation='relu'),  # ノード数が256の層を追加。活性化関数はReLU。\n",
    "    Dense(units=256, activation='relu'),  # ノード数が256の層を追加。活性化関数はReLU。\n",
    "    Dense(units=1, activation='sigmoid')  # ノード数が1の層を追加。活性化関数はシグモイド関数。\n",
    "])\n",
    "\n",
    "#  誤差関数としてクロスエントロピーを指定。最適化手法はadam\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "#  トレーニング\n",
    "model.fit(\n",
    "    x=x,\n",
    "    y=t,\n",
    "    batch_size=1024,  # バッチサイズ。一回のステップで1024行のデータを使うようにする。\n",
    "    epochs=10,  # 学習のステップ数\n",
    "    verbose=1,  # 1とするとステップ毎に誤差関数の値などが表示される\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "計算時間はどのように変化したでしょうか。\n",
    "モデルサイズが変化すると、計算時間はどのように変わるでしょうか？"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
