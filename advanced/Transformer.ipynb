{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8029bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorchが使うCPUの数を制限します。(VMを使う場合)\n",
    "%env OMP_NUM_THREADS=1\n",
    "%env MKL_NUM_THREADS=1\n",
    "\n",
    "from torch import set_num_threads, set_num_interop_threads\n",
    "num_threads = 1\n",
    "set_num_threads(num_threads)\n",
    "set_num_interop_threads(num_threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007b11de",
   "metadata": {},
   "source": [
    "# Transformerを用いた文字列予測\n",
    "\n",
    "Language Modelはこれまでの単語列から次の単語を予測します。例えば、\n",
    "```\n",
    "P(\"pen\"|\"This is a\") = 0.9\n",
    "P(\"pineapple\"|\"This is a\") = 0.05\n",
    "```\n",
    "のようなイメージです。\"This is a\"ときたら次の単語は\"pen\"である確率が90%ということを意味しています。(90%はテキトーな値です。実際はもっと低いと思います。)\n",
    "\n",
    "このノートブックではTransformerを使って、このような言語モデル(Language Model)を実装してみましょう。\n",
    "通常はトークン(\"単語\"のような単位)を予測しますが、ここでは簡単のため、\"文字\"を予測させるようにしましょう。\n",
    "例\n",
    "```\n",
    "P(\"e\"|\"This is a p\") = 0.8\n",
    "P(\"i\"|\"This is a p\") = 0.03\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1935d7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b40c84d",
   "metadata": {},
   "source": [
    "まずは文字を数字に直します。ここではアスキーコードを使うことにしましょう。\n",
    "| 文字 | ASCIIコード |\n",
    "| --- | --- |\n",
    "| A | 65 |\n",
    "| B | 66 |\n",
    "| ... | ... |\n",
    "| Z | 90 |\n",
    "| a | 91 |\n",
    "| ... | ... |\n",
    "| z | 122 |\n",
    "\n",
    "このトークン化をすると、\"Hello World!\"は\n",
    "\n",
    "`['H', 'e', 'l', 'l', 'o', ' ', 'W', 'o', 'r', ',', 'd', '!']`\n",
    "\n",
    "->\n",
    "\n",
    "`[72, 101, 108, 108, 111, 32, 87, 111, 114, 108, 100, 33]`\n",
    "\n",
    "と変換されます。('H' = 77, 'e' = 101, ...)\n",
    "\n",
    "この操作は以下のようにして関数化出来ます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d71ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 128  # アスキー文字の数\n",
    "def encode(s): return [ord(c) for c in s]\n",
    "def decode(l): return ''.join([chr(i) for i in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1e7012",
   "metadata": {},
   "outputs": [],
   "source": [
    "encode(\"Hello World!\")  # ['H', 'e', 'l', 'l', 'o', ' ', 'W', 'o', 'r', ',', 'd', '!']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccaa4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "decode([72, 101, 108, 108, 111, 32, 87, 111, 114, 108, 100, 33])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcba29f1",
   "metadata": {},
   "source": [
    "## データセット\n",
    "\n",
    "データセットは\"Tiny Shakespeare\"を使ってみます。\n",
    "これはシェイクスピアの台詞をデータセットとしてまとめたものです。\n",
    "\n",
    "[tinyshakespeare](\"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\")から取得できます。\n",
    "今回の実習環境では`/data/staff/deeplearning/tinyshakespeare.txt`にもあります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb45e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/data/staff/deeplearning/tinyshakespeare.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# 空行や役名、改行を除外します。\n",
    "text = \" \".join([\n",
    "    line\n",
    "    for line in text.split(\"\\n\")\n",
    "    if not (line == '' or line.endswith(\":\"))\n",
    "])\n",
    "\n",
    "print(text[:500])  # 最初の500文字を表示\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)  # Torchのテンソルに変換"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfbd0de",
   "metadata": {},
   "source": [
    "上記の処理で、役名や改行などを除去したので、文同士のつながりが分かりづらくなってしまっていますが、単語や文法を学ぶには問題ないでしょう。\n",
    "\n",
    "このデータセットを使って、次文字予測をします。(例: \"Befor\" -> \"e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae74bca",
   "metadata": {},
   "source": [
    "データを効率的に読み出すためにデータセットクラスを定義します。\n",
    "ここでは`context_size`ごとにテキストを切り取っています。　\n",
    "元のテキストは\"Before we proceed any further ...\"でしたが、ここから\n",
    "| 入力 (x) | 出力ラベル (t) |\n",
    "| --- | --- |\n",
    "| Before we  | efore we p |\n",
    "| efore we p | fore we pr |\n",
    "| fore we pr | ore we pro |\n",
    "| ore we pro | re we proc |\n",
    "| re we proc | e we procc |\n",
    "\n",
    "のような入力・出力の組み合わせを作成しています。\n",
    "このトレーニングデータを使って\n",
    "\"Before we \"が入力されたときに\"p\"を出力するようなモデルを学習させます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8d9e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, data, context_size):\n",
    "        self.data = data\n",
    "        self.context_size = context_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.context_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx : idx + self.context_size]\n",
    "        y = self.data[idx + 1 : idx + self.context_size + 1]\n",
    "        return x, y\n",
    "    \n",
    "context_size = 10 # 入力長（コンテキスト長）\n",
    "dataset = CharDataset(data, context_size)\n",
    "\n",
    "# データローダーの作成 (ミニバッチで学習を行います。)\n",
    "from torch.utils.data import DataLoader\n",
    "batch_size = 128 # バッチサイズ\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77924763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データセットのサンプルを表示\n",
    "for i in range(10):\n",
    "    x, y = dataset[i]\n",
    "    print(f\"Input: {decode(x.tolist())}, Target: {decode(y.tolist())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01738ed1",
   "metadata": {},
   "source": [
    "TransformerモデルはPyTorchに標準で実装されています。これを用いて`MyTransformer`という名前のモジュールを実装します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4cee79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------\n",
    "# Transformer モデル定義\n",
    "# ---------------------\n",
    "class MyTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size=16, nhead=1, nhid=32, nlayers=1, context_size=10):\n",
    "        super().__init__()\n",
    "\n",
    "        self.context_size = context_size\n",
    "\n",
    "        # 文字(ASCIIコード)を潜在空間上のベクトルに変換するための埋め込み層\n",
    "        self.token_emb = nn.Embedding(vocab_size, emb_size)\n",
    "\n",
    "        # 位置エンコーディングのための学習可能パラメータ\n",
    "        self.pos_emb = nn.Parameter(torch.zeros(1, context_size, emb_size))\n",
    "\n",
    "        # Transformerエンコーダー層の定義\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=emb_size,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=nhid,\n",
    "            batch_first=True\n",
    "        )\n",
    "        # Transformerエンコーダーの定義 (エンコーダー層をスタックして構成)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer=encoder_layer,\n",
    "            num_layers=nlayers\n",
    "        )\n",
    "        # 出力層: Transformerの出力を文字(ASCIIコード)に変換する線形層\n",
    "        self.linear = nn.Linear(emb_size, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T = x.size()\n",
    "\n",
    "        if T > self.context_size:\n",
    "            raise ValueError(f\"Input sequence length ({T}) exceeds context_size ({self.context_size}).\")\n",
    "\n",
    "        x = self.token_emb(x) + self.pos_emb[:, :T, :]\n",
    "\n",
    "        # マスクを生成: Transformerの入力に対して、未来の情報を見えないようにするためのマスクを生成\n",
    "        mask = torch.nn.Transformer.generate_square_subsequent_mask(T, device=x.device)\n",
    "\n",
    "        # Transformerエンコーダーに入力を通す\n",
    "        out = self.transformer_encoder(src=x, mask=mask)\n",
    "\n",
    "        # 出力層を通して、各トークンの次の文字(ASCIIコード)を予測\n",
    "        return self.linear(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6c554a",
   "metadata": {},
   "source": [
    "定義した`MyTransformer`をインスタンス化し、中身を見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6742bdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルのインスタンス化。語彙サイズは128, context_sizeは10、エンコーダ層のレイヤー数は3。\n",
    "model = MyTransformer(vocab_size, context_size=context_size, nlayers=3)\n",
    "\n",
    "import torchinfo\n",
    "torchinfo.summary(model, input_size=(1, context_size), depth=5, col_names=[\"output_size\", \"num_params\"], dtypes=[torch.long])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fcfc33",
   "metadata": {},
   "source": [
    "`MyTransformer`は埋め込み層(Embedding layer)、Transformerエンコーダ層(3層)、出力線形変換、から構成されています。\n",
    "- 埋め込み層は ASCIIコード(0~127)を(ここでは)16次元のベクトルに変換します。すなわち文字の一つ一つが16次元のベクトルに変換されます。Transformer内部ではこの16次元のベクトルを変換してきます。\n",
    "  - 例:\n",
    "    - 'a' -> (0.01,  0.02, ..., 0.01)\n",
    "    - 'b' -> (0.00,  0.05, ..., 0.02)\n",
    "    - 'c' -> (0.00, -0.02, ..., 0.03)\n",
    "    - (ここではこれらのベクトルは学習可能なパラメータとしているので、学習で最適化されます。)\n",
    "- 最後のLinear層は16次元のベクトルをASCIIコード(0~127)に変換します。正確には、各文字に対応する確率が出力されます('a'の確率が32%、'b'の確率が2%、といったイメージです。)。最も確率の高い文字を予測値として用います。\n",
    "  - 例:\n",
    "    - ('a', 'b', 'c', ...) = (0.02, 0.10, 0.85, ...) のような出力のとき、最大の値を持つ'c'が予測値となります。\n",
    "- Transformerエンコーダ層は複数のモジュールから構成されていますが、大雑把には`Self-Attention` -> `MLP` の2つで構成されています。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffccf8e",
   "metadata": {},
   "source": [
    "予測(生成)は1文字ずつ実行します。例えば\"This\"から始めて文字列生成をすると、\n",
    "1. Model('This ') -> 'i'\n",
    "2. Model('This i') -> 's'\n",
    "3. Model('This is') -> ' ' (スペース)\n",
    "4. Model('This is ') -> 'a'\n",
    "\n",
    "のように、1文字ずつ、かつそれまでの予測結果を使いながら次の文字を予測してきます。\n",
    "\n",
    "以下の関数は上記の操作を実行する関数です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f42a89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------\n",
    "# 推論\n",
    "# ---------------------\n",
    "def generate(model, start_text=\"\", context_size=5, length=5):\n",
    "    # モデルを評価モードに設定\n",
    "    model.eval()\n",
    "\n",
    "    # 入力文字をASCIIコードにエンコードした後、PyTorch Tensor型に変換\n",
    "    context = torch.tensor(encode(start_text), dtype=torch.long).unsqueeze(0)  # (1, T)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # length個の文字を生成する\n",
    "        for _ in range(length):\n",
    "            # 順伝搬。文字ごとの出現確率を予測\n",
    "            logits = model(context[:, -context_size:])\n",
    "\n",
    "            # 予測した出現確率から、最も高い確率の文字を選ぶ\n",
    "            next_token = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
    "\n",
    "            # 予測した文字をテキストに追加する\n",
    "            context = torch.cat([context, next_token], dim=1)\n",
    "\n",
    "    return decode(context[0].tolist())  # contextをデコード (ASCIIコード -> 文字列)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9894a5",
   "metadata": {},
   "source": [
    "学習前のモデルに対して生成を実行してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3072ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 30文字文のテキストを生成。\n",
    "generate(model, \"This\", context_size=context_size, length=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c2671c",
   "metadata": {},
   "source": [
    "学習前なのでランダムな文字列が出力されています。\n",
    "\n",
    "それではモデルの学習をさせてみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463807ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------\n",
    "# 学習\n",
    "# ---------------------\n",
    "from tqdm import tqdm\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# for i_epoch in range(1):\n",
    "model.train()\n",
    "\n",
    "for x_batch, y_batch in tqdm(dataloader):\n",
    "\n",
    "    # 順伝搬\n",
    "    # - 入力のshape: (バッチサイズ, トークン長)\n",
    "    # - 出力のshape: (バッチサイズ, トークン長, 語彙数(=128))\n",
    "    logits = model(x_batch)\n",
    "    \n",
    "    # ロスの計算 (クロスエントロピー損失を使用)\n",
    "    loss = F.cross_entropy(logits.view(-1, vocab_size), y_batch.view(-1))\n",
    "\n",
    "    # 誤差逆伝播の前に各パラメータの勾配の値を0にセットする。\n",
    "    # これをしないと、勾配の値はそれまでの値との和がとられる。\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 誤差逆伝播。各パラメータの勾配が計算される。\n",
    "    loss.backward()\n",
    "\n",
    "    # 各パラメータの勾配の値を基に、optimizerにより値が更新される。\n",
    "    optimizer.step()\n",
    "\n",
    "print(f\"loss = {loss}\")\n",
    "# print(f\"epoch = {i_epoch}, loss = {loss}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cb3b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 30文字文のテキストを生成。\n",
    "generate(model, \"This\", context_size=context_size, length=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df80cb19",
   "metadata": {},
   "source": [
    "意味のある\"文\"は予測できていませんが、学習前と比較すると\"単語\"っぽいものは生成できるようになったと思います。\n",
    "\n",
    "LLM (Large Language Model)もここで実装したものと同じような仕組みで単語を出力させています。自然な文章を出力させるためには、より大きなモデル、より大きなデータセット、膨大な計算資源が必要となります。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
