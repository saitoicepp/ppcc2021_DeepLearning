{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "多変量解析TMVA講習で学んだように、Machine Learningについての知識は、実験データを解析する上で欠かせないものになりつつあります。特に近年Deep learning(深層学習)が急速に発展してきており、実験物理分野でも用いられるようになりました。\n",
    "\n",
    "Deep learningを扱うツールが整備されたことにより、誰でも簡単にDeep learningが使えるようになりました。一方で、その背後にある仕組みを知らないまま扱うと、落とし穴にハマることがあるかもしれません。\n",
    "\n",
    "この講義では、Deep learningの基礎、特にDeep learningのベースとなるパーセプトロンを学んでいきます。式を追うだけでは理解しきれないことも多いと思うので、サンプルコードをぜひ自分の手で動かしてみてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter notebookについて\n",
    "この講義ではjupyter notebookを提供します。\n",
    "数式と文章で解説をし、要所要所で式やアルゴリズムを理解するコードがpythonで書かれています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## パッケージのインポート\n",
    "まず始めに、必要なパッケージをインポートします。\n",
    "numpyは多次元配列を効率的に行うパッケージで、matplotlibはプロット等を行うパッケージです。\n",
    "\"%matplotlib inline\"はjupyter notebookでプロットを行う際に必要ですが、通常のpython scriptとしてコードを実行する際は必要ありません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ライブラリのインポート\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# パーセプトロン (Perceptron)\n",
    "(単純)パーセプトロンは1957年に開発されたアルゴリズムで、分類問題を解くことができます。\n",
    "\n",
    "分類問題とは、入力を複数のクラスにラベル付けすることです。物理実験分野ではシグナルとバックグラウンドの分類としてよく用いられます。\n",
    "\n",
    "例えば、n個の入力変数$ \\mathbf{x}=\\{x_1, x_2,\\dots,x_n\\} $からクラス$ t\\in\\{0, 1\\} $を推定する分類問題を考えると、入力がシグナル由来のものならば\n",
    "$$ t = y(\\mathbf{x}_S) = 1 $$\n",
    "入力がバックグラウンド由来のものならば\n",
    "$$t = y(\\mathbf{x}_B) = 0 $$\n",
    "となるような関数$y(\\mathbf{x})$を作ることが目的になります。\n",
    "\n",
    "パーセプトロンでは、関数を以下のように定義します。\n",
    "$$\n",
    "\\begin{align*}\n",
    "y(\\mathbf{x}|\\mathbf{w}, b) &= \\Gamma(\\mathbf{w}\\cdot \\mathbf{x} + b) \\\\\n",
    "\\Gamma(a) &= \\begin{cases}\n",
    "    1 & (a\\geq0)\\\\\n",
    "    0 & (a<0)\n",
    "  \\end{cases}\n",
    "\\end{align*}\n",
    "$$\n",
    "ここで、$ \\mathbf{w},b $は関数のパラメータ、$ \\Gamma $はステップ関数です。関数のパラメータを問題によって調整することで、分類問題を解くことができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANDゲート\n",
    "まず初めに2変数( $x_1, x_2$ )を入力とするパーセプトロンを用いてANDゲートを実装してみましょう。\n",
    "\n",
    "ANDゲートは\n",
    "             \n",
    "| $x_1$ | $x_2$ | AND |\n",
    "| :-: | :-: | :-: |\n",
    "| 0 | 0 | 0 |\n",
    "| 0 | 1 | 0 |\n",
    "| 1 | 0 | 0 |\n",
    "| 1 | 1 | 1 |\n",
    "\n",
    "のように、2つの入力変数がともに1のときのみ1を返し、それ以外のときは0を返すような関数です。\n",
    "入力変数が2つなので、パーセプトロン関数を明示的に書くと、\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y(x_1,x_2|w_1,w_2, b) &= \\Gamma(w_1 x_1 + w_2 x_2 + b) \\\\\n",
    "\\Gamma(a) &= \\begin{cases}\n",
    "    1 & (a\\geq0)\\\\\n",
    "    0 & (a<0)\n",
    "  \\end{cases}\n",
    "\\end{aligned}\n",
    "$$\n",
    "となります。これをpythonで書くと以下のようになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def and_gate(x1, x2):\n",
    "    w1 = 1.0  # FIXME: 適切なパラメータの値を入力してください\n",
    "    w2 = 2.0  # FIXME: 適切なパラメータの値を入力してください\n",
    "    b = 3.0  # FIXME: 適切なパラメータの値を入力してください\n",
    "    a = w1 * x1 + w2 * x2 + b\n",
    "    if a >= 0.:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "# 正しく実装できているかを出力\n",
    "print(\"AND Gate:\")\n",
    "print(\"  x1, x2, y\")\n",
    "print(\"   0,  0, \", and_gate(0, 0), \"Correct\" if and_gate(0, 0) == 0 else \"Wrong\")  # 正解は 0\n",
    "print(\"   0,  1, \", and_gate(0, 1), \"Correct\" if and_gate(0, 1) == 0 else \"Wrong\")  # 正解は 0\n",
    "print(\"   1,  0, \", and_gate(1, 0), \"Correct\" if and_gate(1, 0) == 0 else \"Wrong\")  # 正解は 0\n",
    "print(\"   1,  1, \", and_gate(1, 1), \"Correct\" if and_gate(1, 1) == 1 else \"Wrong\")  # 正解は 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上の例ではパラメータが適当なため、正しいANDゲートにはなっていません。パラメータ($w_1, w_2, b$)をどのように設定すれば、この関数がANDゲートとなるでしょうか？\n",
    "\n",
    "実は答えは一意には決まらず無数にあるのですが、例えば$(w_1, w_2, b) = (1, 1, -1.5)$とするとうまくいきます。確認してみてください。\n",
    "\n",
    "パーセプトロンの値をプロットすると\n",
    "<img src=\"jupyterFigure/AND.png\" width=\"400\">\n",
    "となります。\n",
    "赤い領域で$y=1$、青い領域で$y=0$を出力しています。\n",
    "パーセプトロンの式の形からもわかるように、$(x_1, x_2)$平面上を直線($w_1 x_1 + w_2 x_2 + b=0$)で区切ったような出力となります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NANDゲート・ORゲート\n",
    "NANDゲートやORゲートは\n",
    "\n",
    "| $x_1$ | $x_2$ | NAND | OR |\n",
    "| -- | -- | -- | -- |\n",
    "| 0 | 0 | 1 | 0 |\n",
    "| 0 | 1 | 1 | 1 |\n",
    "| 1 | 0 | 1 | 1 |\n",
    "| 1 | 1 | 0 | 1 |\n",
    "\n",
    "のような関数です。各自どのようにパラメータを決めればよいか確認してみてください。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def nand_gate(x1, x2):\n",
    "    w1 = 1.0  # FIXME: 適切なパラメータの値を入力してください\n",
    "    w2 = 2.0  # FIXME: 適切なパラメータの値を入力してください\n",
    "    b = 3.0  # FIXME: 適切なパラメータの値を入力してください\n",
    "    a = w1 * x1 + w2 * x2 + b\n",
    "    if a >= 0.:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def or_gate(x1, x2):\n",
    "    w1 = 1.0  # FIXME: 適切なパラメータの値を入力してください\n",
    "    w2 = 2.0  # FIXME: 適切なパラメータの値を入力してください\n",
    "    b = 3.0  # FIXME: 適切なパラメータの値を入力してください\n",
    "    a = w1 * x1 + w2 * x2 + b\n",
    "    if a >= 0.:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "# 正しく実装できているかを出力\n",
    "print(\"NAND Gate:\")\n",
    "print(\"  x1, x2, y\")\n",
    "print(\"   0,  0, \", nand_gate(0, 0), \"Correct\" if nand_gate(0, 0) == 1 else \"Wrong\")  # 正解は 1\n",
    "print(\"   0,  1, \", nand_gate(0, 1), \"Correct\" if nand_gate(0, 1) == 1 else \"Wrong\")  # 正解は 1\n",
    "print(\"   1,  0, \", nand_gate(1, 0), \"Correct\" if nand_gate(1, 0) == 1 else \"Wrong\")  # 正解は 1\n",
    "print(\"   1,  1, \", nand_gate(1, 1), \"Correct\" if nand_gate(1, 1) == 0 else \"Wrong\")  # 正解は 0\n",
    "\n",
    "print(\"OR Gate:\")\n",
    "print(\"  x1, x2, y\")\n",
    "print(\"   0,  0, \", or_gate(0, 0), \"Correct\" if or_gate(0, 0) == 0 else \"Wrong\")  # 正解は 0\n",
    "print(\"   0,  1, \", or_gate(0, 1), \"Correct\" if or_gate(0, 1) == 1 else \"Wrong\")  # 正解は 1\n",
    "print(\"   1,  0, \", or_gate(1, 0), \"Correct\" if or_gate(1, 0) == 1 else \"Wrong\")  # 正解は 1\n",
    "print(\"   1,  1, \", or_gate(1, 1), \"Correct\" if or_gate(1, 1) == 1 else \"Wrong\")  # 正解は 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ロジスティック回帰 (Logistic regression)\n",
    "これまで見てきた３つのゲート(AND,OR,NAND)は、出力が一意に定まる問題でした。一方で現実の問題のほとんどは一意にラベル付けはできません。例えば($H\\rightarrow\\gamma\\gamma$)のように、観測された事象がシグナルであるのかバックグラウンドであるのかは確率的にしか評価できないことがほとんどです。\n",
    "<img src=\"jupyterFigure/Hgg.png\" width=\"300\">\n",
    "そのため、関数の出力を0 or 1とするのではなく、0から1まで連続的な値を取るように変更します。ここで出力は、クラスのラベル(S or B)ではなく、そのラベルを取る確率を表します。\n",
    "出力として0 or 1を取るステップ関数の代わりによく使われる関数がシグモイド関数です。\n",
    "$$ \\sigma(x) = \\frac{1}{1+e^{-x}} $$\n",
    "<img src=\"jupyterFigure/sigmoid.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x\\rightarrow \\pm \\infty$の極限では、ステップ関数もシグモイド関数も同じ値となりますが、x=0付近で、シグモイド関数はなめらかになっており、0から1まで連続的な値を取ることができます。また、x軸のスケールを変更すると、シグモイド関数はステップ関数に限りなく近づくという性質もあります。\n",
    "\n",
    "パーセプトロンの式は変更されて、\n",
    "$$\n",
    "\\begin{align*}\n",
    "y(\\mathbf{x}|\\mathbf{w}, b) &= \\sigma(\\mathbf{w}\\cdot \\mathbf{x} + b) \\\\\n",
    "\\sigma(x) &= \\frac{1}{1+e^{-x}}\n",
    "\\end{align*}\n",
    "$$\n",
    "となります。先程の式のステップ関数の部分がシグモイド関数に変わっています。\n",
    "この関数を用いて分類・回帰をすることをロジスティック回帰(Logistic regression)と呼ぶこともありますし、パーセプトロンと呼ぶこともあります。ここではパーセプトロンと呼びます。\n",
    "\n",
    "なお、このステップ関数やシグモイド関数に対応する部分を__活性化関数(activation function)__とも呼びます。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実例\n",
    "この式を用いて、実際に問題を解いてみましょう。\n",
    "\n",
    "次のセルはヘルパー関数です。データ点の作成や、プロットを行います。内容は完全に理解する必要はありませんが、もし余力があればコードを追ってみてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 中心値の異なる2つの二次元ガウス分布\n",
    "def get_dataset_1():\n",
    "    from numpy.random import default_rng\n",
    "    rng = default_rng(seed=0)  # 今回はデータセットの乱数を固定させます。\n",
    "\n",
    "    num_signal = 100  # 生成するシグナルイベントの数\n",
    "    num_background = 100  # 生成するバックグラウンドイベントの数\n",
    "\n",
    "    # データ点の生成\n",
    "    ## 平均(x1,x2) = (1.0, 0.0)、分散=1の２次元ガウス分布\n",
    "    x_sig = rng.multivariate_normal(mean=[1.0, 0.0],\n",
    "                                    cov=[[1., 0.], [0., 1.]],\n",
    "                                    size=num_signal)\n",
    "    t_sig = np.ones((num_signal, 1))  # Signalは1にラベリング\n",
    "\n",
    "    ## 平均(x1,x2) = (-1.0, 0.0)、分散=1の２次元ガウス分布\n",
    "    x_bg = rng.multivariate_normal(mean=[-1.0, 0.0],\n",
    "                                   cov=[[1., 0.], [0., 1.]],\n",
    "                                   size=num_background)\n",
    "    t_bg = np.zeros((num_background, 1))  # Backgroundは0にラベリング\n",
    "\n",
    "    # 2つのラベルを持つ学習データを1つにまとめる\n",
    "    x = np.concatenate([x_sig, x_bg])\n",
    "    t = np.concatenate([t_sig, t_bg])\n",
    "\n",
    "    # データをランダムに並び替える\n",
    "    p = rng.permutation(len(x))\n",
    "    x, t = x[p], t[p]\n",
    "\n",
    "    return x, t\n",
    "\n",
    "\n",
    "# 二次元ガウス分布と一様分布\n",
    "def get_dataset_2():\n",
    "    from numpy.random import default_rng\n",
    "    rng = default_rng(seed=0)  # 今回はデータセットの乱数を固定させます。\n",
    "\n",
    "    num_signal = 100  # 生成するシグナルイベントの数\n",
    "    num_background = 1000  # 生成するバックグラウンドイベントの数\n",
    "\n",
    "    # データ点の生成\n",
    "    ## 平均(x1,x2) = (1.0, 0.0)、分散=1の２次元ガウス分布\n",
    "    x_sig = rng.multivariate_normal(mean=[1.0, 0],\n",
    "                                    cov=[[1, 0], [0, 1]],\n",
    "                                    size=num_signal)\n",
    "    t_sig = np.ones((num_signal, 1))  # Signalは1にラベリング\n",
    "\n",
    "    ## (-5, +5)の一様分布\n",
    "    x_bg = rng.uniform(low=-5, high=5, size=(num_background, 2))\n",
    "    t_bg = np.zeros((num_background, 1))  # Backgroundは0にラベリング\n",
    "\n",
    "    # 2つのラベルを持つ学習データを1つにまとめる\n",
    "    x = np.concatenate([x_sig, x_bg])\n",
    "    t = np.concatenate([t_sig, t_bg])\n",
    "\n",
    "    # データをランダムに並び替える\n",
    "    p = rng.permutation(len(x))\n",
    "    x, t = x[p], t[p]\n",
    "\n",
    "    return x, t\n",
    "\n",
    "\n",
    "# ラベル t={0,1}を持つデータ点のプロット\n",
    "def plot_datapoint(x, t):\n",
    "    # シグナル/バックグラウンドの抽出\n",
    "    xS = x[t[:, 0] == 1]  # シグナルのラベルだけを抽出\n",
    "    xB = x[t[:, 0] == 0]  # バックグラウンドのラベルだけを抽出\n",
    "\n",
    "    # プロット\n",
    "    plt.scatter(xS[:, 0], xS[:, 1],label='Signal', c='red', s=10)  # シグナルをプロット\n",
    "    plt.scatter(xB[:, 0], xB[:, 1], label='Background', c='blue', s=10)  # バックグラウンドをプロット\n",
    "    plt.xlabel('x1')  # x軸ラベルの設定\n",
    "    plt.ylabel('x2')  # y軸ラベルの設定\n",
    "    plt.legend()  # legendの表示\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# prediction関数 の等高線プロット (fill)\n",
    "def plot_prediction(prediction, *args):\n",
    "    # 等高線を描くためのメッシュの生成\n",
    "    x1, x2 = np.mgrid[-5:5:100j, -5:5:100j]  # x1 = (-5, 5), x2 = (-5, 5) の範囲で100点x100点のメッシュを作成\n",
    "    x1 = x1.flatten()  # 二次元配列を一次元配列に変換 ( shape=(100, 100) => shape=(10000, ))\n",
    "    x2 = x2.flatten()  # 二次元配列を一次元配列に変換 ( shape=(100, 100) => shape=(10000, ))\n",
    "    x = np.array([x1, x2]).T\n",
    "\n",
    "    #  関数predictionを使って入力xから出力yを計算し、等高線プロットを作成\n",
    "    y = prediction(x, *args)\n",
    "    cs = plt.tricontourf(x[:, 0], x[:, 1], y.flatten(), levels=10)\n",
    "    plt.colorbar(cs)\n",
    "\n",
    "\n",
    "# prediction関数 の等高線プロット (line)\n",
    "def plot_prediction_line(prediction, *args):\n",
    "    # 等高線を描くためのメッシュの生成\n",
    "    x1, x2 = np.mgrid[-5:5:100j, -5:5:100j]  # x1 = (-5, 5), x2 = (-5, 5) の範囲で100点x100点のメッシュを作成\n",
    "    x1 = x1.flatten()  # 二次元配列を一次元配列に変換 ( shape=(100, 100) => shape=(10000, ))\n",
    "    x2 = x2.flatten()  # 二次元配列を一次元配列に変換 ( shape=(100, 100) => shape=(10000, ))\n",
    "    x = np.array([x1, x2]).T\n",
    "\n",
    "    #  関数predictionを使って入力xから出力yを計算し、等高線プロットを作成\n",
    "    y = prediction(x, *args)\n",
    "    plt.tricontour(x[:, 0], x[:, 1], y.flatten(), levels=10)\n",
    "\n",
    "\n",
    "def plot_prediction_regression(x, t, prediction, w1, b1, w2, b2):\n",
    "    def _sigmoid(x):\n",
    "        return 1. / (1 + np.exp(-x))\n",
    "\n",
    "    def _perceptron(x, w, b):\n",
    "        a = np.dot(x, w) + b  # w・x + b\n",
    "        return _sigmoid(a)\n",
    "\n",
    "    #  データ点のプロット\n",
    "    plt.scatter(x, t, s=10, c='black')\n",
    "\n",
    "    #  関数predictionの出力をプロット\n",
    "    y = prediction(x, w1, b1, w2, b2)\n",
    "    plt.plot(x, y, c='red')\n",
    "\n",
    "    # 中間層の各ノードの出力をプロット\n",
    "    num_nodes = len(w2)\n",
    "    for i in range(num_nodes):\n",
    "        y = w2[i] * _perceptron(x, w1[:, i], b1[i])\n",
    "        plt.plot(x, y, linestyle='dashed')  # (中間層のノードの出力 * 重み)をプロット\n",
    "    plt.plot(x, np.full_like(x, b2[0]), linestyle='dashed')  # 最後の層のバイアスタームのプロット\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上の関数を使ってデータ点をプロットしましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ点の取得\n",
    "x, t = get_dataset_1()\n",
    "\n",
    "# データ点をプロット\n",
    "plot_datapoint(x, t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このようなシグナル・バックグラウンドを分類することを考えます。この分類問題では、シグナルとバックグラウンドの分布が一部重なっています。したがって、そのような中間部分では出力0 or 1ではなく、そのイベントがシグナルである確率を返すように、シグモイド関数を使います。\n",
    "\n",
    "パーセプトロンの式は"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1. / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def perceptron(x1, x2, w1, w2, b):\n",
    "    a = w1 * x1 + w2 * x2 + b\n",
    "    return sigmoid(a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "となります。最後の部分だけがステップ関数からシグモイド関数に変わっています。\n",
    "より一般的にnumpyのarrayを用いてベクトル的に書くと、"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1. / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def perceptron(x, w, b):\n",
    "    a = np.dot(x, w) + b  # w・x + b\n",
    "    return sigmoid(a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "のようになります。ここで、x,wはベクトルで、np.dot(x,w)はxとwの内積を取る操作をします。\n",
    "\n",
    "試しに、入力変数($x$)やパラメータ$(w,b)$を変化させてどのような値が返ってくるのか確認してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = np.array([1.0, 2.0])\n",
    "w = np.array([1.0, 1.0])\n",
    "b = np.array(-1.5)\n",
    "\n",
    "print(f\"w1 * x1 + w2 * x2 + b = {w[0]} * {x[0]} + {w[1]} * {x[1]} + {b}\")\n",
    "print(f\"                      = {perceptron(x, w, b)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{w}\\cdot \\mathbf{x} + b &= \n",
    "\\begin{pmatrix}\n",
    "w_1 & w_2\n",
    "\\end{pmatrix}\n",
    "\\cdot\n",
    "\\begin{pmatrix}\n",
    "x_1 \\\\\n",
    "x_2\n",
    "\\end{pmatrix}\n",
    "+ b \\\\\n",
    "&=\n",
    "\\begin{pmatrix}\n",
    "1.0 & 1.0\n",
    "\\end{pmatrix}\n",
    "\\cdot\n",
    "\\begin{pmatrix}\n",
    "1.0 \\\\\n",
    "2.0\n",
    "\\end{pmatrix}\n",
    "-1.5  = 1.5 \\\\\n",
    "\\sigma(1.5) &= \\frac{1}{1+e^{-1.5}} = 0.818\n",
    "\\end{align}\n",
    "$$\n",
    "を計算しています。\n",
    "\n",
    "xには複数の入力を与えることもできます。xのshapeを(データセットの数, 変数の数)、wのshapeを(変数の数)とすれば、perceptronの式の np.dot(x, w) の返り値のshapeは(データセットの数, )となります。\n",
    "\n",
    "例えば次のようにして、一回の計算で複数のデータ点を処理できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([\n",
    "    [0.0, 0.0],  # event 1\n",
    "    [0.0, 1.0],  # event 2\n",
    "    [1.0, 0.0],  # event 3\n",
    "    [1.0, 1.0],  # event 4\n",
    "])\n",
    "w = np.array([1.0, 1.0])\n",
    "b = np.array(-1.5)\n",
    "\n",
    "print(perceptron(x, w, b))  # 4イベントの出力がまとめて返される\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先程定義したヘルパー関数を用いて等高線プロットを作ると、"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パーセプトロンのパラメータ\n",
    "w = np.array([1.0, 1.0])  # FIXME: パラメータの値を適当に変更してみましょう\n",
    "b = np.array(1.5)  # FIXME: パラメータの値を適当に変更してみましょう\n",
    "\n",
    "# パーセプトロンの出力を等高線プロット\n",
    "plot_prediction(perceptron, w, b)\n",
    "\n",
    "# データ点の取得\n",
    "x, t = get_dataset_1()\n",
    "\n",
    "# データ点をプロット\n",
    "plot_datapoint(x, t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "となります。先程のステップ関数を使った例とは異なり、なめらかな出力をしていることがわかります。\n",
    "パラメータの値を変化させて、パーセプトロンの出力がどのように変化するのか確かめてみましょう。\n",
    "\"最適な\"パラメータの値は何になるでしょうか？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## パーセプトロンのグラフ表現\n",
    "パーセプトロンの式をグラフで表すと視覚的にわかりやすく、また後々より複雑なモデルを作るときに、全体像を把握しやすくなります。\n",
    "パーセプトロンの式は\n",
    "$$\n",
    "\\begin{align*}\n",
    "y(\\mathbf{x}|\\mathbf{w}, b) &= \\sigma(\\mathbf{w}\\cdot \\mathbf{x} + b)\n",
    "\\end{align*}\n",
    "$$\n",
    "でしたが、これをさらに分解して、各成分を明示的に書くと、\n",
    "$$\n",
    "\\begin{align*}\n",
    "a &= w_1 \\cdot x_1 + w_2 \\cdot x_2 + b \\\\\n",
    "y &= \\sigma(a)\n",
    "\\end{align*}\n",
    "$$\n",
    "となります。この式を\n",
    "<img src=\"jupyterFigure/perceptron_graph.png\" width=\"300\">\n",
    "のように表現します。グラフにすることによって、入力変数($(x_1, x_2)$)が右に伝搬して出力($y$)までどのように計算されるのかがわかりやすくなります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 機械学習\n",
    "これまでは手で関数のパラメータを決めてきましたが、問題が複雑になると、限界がきます。データからモデルのパラメータを自動で決めるのが、機械学習です。適切に学習させることで、人間が手でチューニングしたものよりも良い結果を得ることができます。\n",
    "\n",
    "さて、この学習はどのようにして進めたら良いでしょうか？\n",
    "最良の出力を得るパラメータとはどのようなものでしょうか？\n",
    "\n",
    "ここでまた2クラス分類問題を考えます。n個の入力変数($\\mathbf{x}=\\{x_1, x_2,\\dots,x_n\\}$)からクラス($t\\in\\{0, 1\\}$)を推定します。\n",
    "\n",
    "ある観測値($ \\mathbf{x}=\\{x_1, x_2,\\dots,x_n\\}$)が得られた時、それがシグナルである確率を$y(\\mathbf{x}|\\mathbf{w}, b)$としていました(バックグラウンドである確率は$1-y(\\mathbf{x}|\\mathbf{w}, b)$)。ある観測値の集合が与えられ、それらに対してのラベルがわかっている時、パラメータが観測値を説明する確率は\n",
    "$$\n",
    "p(\\mathbf{t}|\\mathbf{w},b ) = \\prod_{n=1}^{N} y_n^{t_n}\\left(1-y_n \\right)^{1-t_n}\n",
    "$$\n",
    "となります。この式では、$t_n = 1$の時$p_n = y(\\mathbf{x}_n)$、$t_n = 0$の時$p_n=1-y(\\mathbf{x}_n)$となります。\n",
    "\n",
    "最尤法の考え方から、この確率を最大にするモデルパラメータを決定すれば良いことがわかります。負の対数を取ることで、\n",
    "$$\n",
    "E(\\mathbf{w}, b) = -\\log p(\\mathbf{t}|\\mathbf{w}, b) = -\\sum_{n=1}^{N} \\left( t_n \\log y_n + \\left( 1-t_n \\right) \\log \\left( 1-y_n \\right) \\right)\n",
    "$$\n",
    "をパラメータ($\\mathbf{w}, b$)に対して最小化する問題に帰着します。\n",
    "機械学習において最小化する関数のことを__誤差関数(loss function)__と呼びます。\n",
    "また、上のような関数をこの式を__交差エントロピー誤差関数(Cross entropy error function)__と呼びます。\n",
    "この例では誤差関数として交差エントロピー誤差関数を使うということになります。\n",
    "\n",
    "このままでは学習に使うデータ数によって誤差関数のスケールが変わってしまうので、使用するデータ数で規格化したものを実際には使います。\n",
    "$$\n",
    "E(\\mathbf{w}, b) = - \\frac{1}{N} \\sum_{n=1}^{N} \\left( t_n \\log y_n + \\left( 1-t_n \\right) \\log \\left( 1-y_n \\right) \\right)\n",
    "$$\n",
    "\n",
    "ここでは2クラス分類を考えましたが、多クラス分類でも同様に考えることができます。\n",
    "K個のクラス($t\\in\\{0, 1, \\dots, K\\}$)に分類するときは、尤度関数はn番目のイベントの出力のクラス$k$に対する出力\n",
    "$$\n",
    "p(\\mathbf{t}|\\mathbf{w},b ) = \\prod_{n=1}^{N} \\prod_{k=1}^{K} y_{nk}^{t_{nk}}\n",
    "$$\n",
    "となります。ここで$t_{nk}$はn番目のイベントのラベルで、クラスkの時に1,他のクラスのときに0となります(すなわち$\\sum_{k=1}^{K}t_{nk}=1$です)。$y_{nk}$はn番目のイベントのラベルkに対するパーセプトロンの出力です。\n",
    "同様に式変形すると、\n",
    "$$\n",
    "E(\\mathbf{w}, b) = -\\frac{1}{N} \\sum_{n=1}^{N}\\sum_{k=1}^{K} t_{nk} \\log y_{nk}\n",
    "$$\n",
    "となります。この式も同様に交差エントロピー誤差関数(Cross entropy error function)と呼びます。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 最急降下法 (Gradient descent)\n",
    "パーセプトロンのパラメータを最適にする指針(誤差関数)は決めましたが、その最小化方法は自明ではありません。\n",
    "よく用いられる手法は勾配法です。勾配法は、その名の通り、関数の勾配を用いて、その極小値を探索する手法です。\n",
    "ここでは勾配法の最もシンプルな例である最急降下法(Gradient descent)を用いて誤差関数を最小化します。\n",
    "\n",
    "最急降下法では、入力が$\\mathbf{x}=(x_1,\\dots,x_n)$の関数$f(\\mathbf{x})$の最小値を、以下のように逐次的に求めます。\n",
    "$$\n",
    "\\mathbf{x}^{(k+1)} = \\mathbf{x}^{(k)} - \\epsilon \\cdot \\left. \\frac{\\partial f}{\\partial \\mathbf{x}} \\right|_{\\mathbf{x}=\\mathbf{x}^{(k)}}\n",
    "$$\n",
    "ここで$\\epsilon$は1回あたりの更新の大きさを決めるパラメータで、値が小さいと、ゆっくり最小値を探します。\n",
    "関数の一次微分($\\left. \\frac{\\partial f}{\\partial \\mathbf{x}} \\right|_{\\mathbf{x}=\\mathbf{x}^{(k)}}$)は点$\\mathbf{x}^{(k)}$での傾きを表しており、(適切な$\\epsilon$を選べば)必ず関数の値が小さくなるように更新されます。\n",
    "\n",
    "簡単な例でこのアルゴリズムが正しく動作することを確かめてみます。\n",
    "$f(x_1,x_2)=x_1^2 + x_2^2$を最小化する$(x_1, x_2)$を求めます。更新式は\n",
    "$$\n",
    "\\begin{align*}\n",
    "x_0 &\\leftarrow x_0 - \\epsilon \\cdot (2\\cdot x_1) \\\\\n",
    "x_1 &\\leftarrow x_1 - \\epsilon \\cdot (2\\cdot x_2) \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "となります。\n",
    "\n",
    "python codeでは"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初期値の設定\n",
    "x1 = 4.0\n",
    "x2 = 3.0\n",
    "\n",
    "learning_rate = 0.1  # ステップ幅\n",
    "num_steps = 10  # 繰り返し回数\n",
    "for _ in range(num_steps):\n",
    "    # 値の更新\n",
    "    x1 = x1 - learning_rate * (2 * x1)\n",
    "    x2 = x2 - learning_rate * (2 * x2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "のようにして値を更新していきます。learning_rateが$\\epsilon$に対応するパラメータです。\n",
    "\n",
    "これを実行し、パラメータ($x_1, x_2$)の値の推移をプロットしてみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 目的関数の等高線プロット\n",
    "x1, x2 = np.mgrid[-5:5:100j, -5:5:100j]  # x1 = (-5, 5), x2 = (-5, 5) の範囲で100点x100点のメッシュを作成\n",
    "y = np.square(x1) + np.square(x2)  # y = x1^2 + x2^2\n",
    "plt.contour(x1, x2, y, linestyles='dashed', levels=5)\n",
    "\n",
    "# 初期値の設定\n",
    "x1 = 4.0\n",
    "x2 = 3.0\n",
    "\n",
    "plt.scatter(x1, x2, c='black')  # 初期値のプロット\n",
    "\n",
    "# 最急降下法の実行\n",
    "learning_rate = 0.1  # ステップ幅、 FIXME: ステップ幅を適当に変更してみましょう\n",
    "num_steps = 10  # 繰り返し回数、 FIXME: 繰り返し回数を適当に変更してみましょう\n",
    "for _ in range(num_steps):\n",
    "    # 値の更新\n",
    "    x1 = x1 - 2 * x1 * learning_rate\n",
    "    x2 = x2 - 2 * x2 * learning_rate\n",
    "\n",
    "    plt.scatter(x1, x2, edgecolors='black', facecolor='None')  # 更新値のプロット\n",
    "\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.xlim([-5, 5])\n",
    "plt.ylim([-5, 5])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初期値(黒丸)から10回分値を更新しています。最小値を取る$(x_1, x_2)=(0, 0)$に近づいていることがわかります。\n",
    "ステップ幅や更新回数を変更して変化を確認してみてください。\n",
    "特にステップ幅の大きさはどの程度が適切となるでしょうか？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 最急降下法を用いたパーセプトロンの学習\n",
    "それではパーセプトロンのパラメータ($\\mathbf{w}, b$)を交差エントロピー\n",
    "$$\n",
    "\\begin{align*}\n",
    "E(\\mathbf{w}, b) &= \\frac{1}{N} \\sum_{n=1}^{N} E_n = -\\frac{1}{N} \\sum_{n=1}^{N} \\left( t_n \\log y_n + \\left( 1-t_n \\right) \\log \\left( 1-y_n \\right) \\right) \\\\\n",
    "y(\\mathbf{x}_n|\\mathbf{w}, b) &= \\sigma(\\mathbf{w}\\cdot \\mathbf{x}_n + b)\n",
    "\\end{align*}\n",
    "$$\n",
    "を目的関数として、最急降下法を用いて最適化してみましょう。\n",
    "\n",
    "誤差関数の微分は、\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial E_n}{\\partial \\mathbf{w}}\n",
    "&= \\frac{\\partial E_n}{\\partial  y_n}\\cdot \\frac{\\partial  y_n}{\\partial a} \\cdot \\frac{\\partial  a}{\\partial \\mathbf{w}}\\\\\n",
    "&= \\left[ \\frac{y_n-t_n}{y_n(1-y_n)}\\right] \\cdot \\left[ y_n(1-y_n) \\right] \\cdot \\left[ \\mathbf{x_n} \\right] \\\\\n",
    "&= \\left(y_n-t_n\\right) \\cdot \\mathbf{x_n} \\\\\n",
    "\\frac{\\partial E_n}{\\partial b}\n",
    "&= \\left(y_n-t_n\\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "となります。ぜひ各自手で計算してみてください。\n",
    "($\\frac{\\partial \\sigma(x)}{\\partial x} = \\sigma \\cdot (1 - \\sigma)$となることを使いました。)\n",
    "\n",
    "一次微分の式を使って最急降下法で誤差関数を最小化します。\n",
    "更新式をpythonで書くと以下のようになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# データ点の取得\n",
    "x, t = get_dataset_1()\n",
    "\n",
    "# 初期値の設定\n",
    "w = np.array([2.0, 0.0])\n",
    "b = np.array(0.1)\n",
    "\n",
    "# 最急降下法での学習\n",
    "learning_rate = 0.1  # ステップ幅\n",
    "num_steps = 1000  # 繰り返し回数\n",
    "for _ in range(num_steps):\n",
    "    y = perceptron(x, w, b)  # パーセプトロンの出力\n",
    "    y = y[:, np.newaxis]  # numpy array の shape を t に合わせる\n",
    "\n",
    "    grad_w = (y - t) * x\n",
    "    grad_b = (y - t) * 1\n",
    "\n",
    "    w = w - learning_rate * grad_w.mean(axis=0)  # 全てのデータ点についての平均値を取る\n",
    "    b = b - learning_rate * grad_b.mean(axis=0)  # 全てのデータ点についての平均値を取る\n",
    "\n",
    "# プロット\n",
    "plot_prediction(perceptron, w, b)\n",
    "\n",
    "## データ点をプロット\n",
    "plot_datapoint(x, t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 単純パーセプトロンの限界\n",
    "ここまで単純パーセプトロンが分類問題に有用であることを見てきました。しかし単純パーセプトロンは特定の問題においてしか有用でないことが知られています。例えば先程AND/OR/NAND回路を見てきましたが、XOR回路においてはどうなるでしょうか？\n",
    "\n",
    "XOR回路は次のような真理値となります。\n",
    "\n",
    "| $x_1$ | $x_2$ | XOR |\n",
    "| -- | -- | -- |\n",
    "| 0 | 0 | 0 |\n",
    "| 0 | 1 | 1 |\n",
    "| 1 | 0 | 1 |\n",
    "| 1 | 1 | 0 |\n",
    "\n",
    "これを単純パーセプトロン\n",
    "$$\n",
    "\\begin{align*}\n",
    "y(x_1,x_2|w_1,w_2, b) &= \\Gamma(w_1 x_1 + w_2 x_2 + b) \\\\\n",
    "\\Gamma(a) &= \\begin{cases}\n",
    "    1 & (a\\geq0)\\\\\n",
    "    0 & (a<0)\n",
    "  \\end{cases}\n",
    "\\end{align*}\n",
    "$$\n",
    "で分類できるでしょうか？ 実はこの問題は分類ができません。式の形からわかるようにパーセプトロンでは線形に分類できる問題にしか対応できません。\n",
    "\n",
    "もう一つの例として、一様分布するバックグラウンド上に、シグナルが二次元ガウス分布する状況を考えましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ点の取得 (x: 入力データ, t: ラベル)\n",
    "x, t = get_dataset_1()\n",
    "\n",
    "# データ点をプロット\n",
    "plot_datapoint(x, t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このような例を、単純パーセプトロンで分類させようとしても、適切な予測ができません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# データ点の取得\n",
    "x, t = get_dataset_2()\n",
    "\n",
    "# 初期値の設定\n",
    "w = np.array([0.1, 0.0])\n",
    "b = np.array(0.0)\n",
    "\n",
    "# 最急降下法での学習\n",
    "learning_rate = 0.1  # ステップ幅\n",
    "num_steps = 1000  # 繰り返し回数\n",
    "for _ in range(num_steps):\n",
    "    y = perceptron(x, w, b)  # パーセプトロンの出力\n",
    "    y = y[:, np.newaxis]  # numpy array の shape を t に合わせる\n",
    "\n",
    "    grad_w = (y - t) * x\n",
    "    grad_b = (y - t) * 1\n",
    "\n",
    "    w = w - learning_rate * grad_w.mean(axis=0)  # 全てのデータセットについての平均値を取る\n",
    "    b = b - learning_rate * grad_b.mean(axis=0)  # 全てのデータセットについての平均値を取る\n",
    "\n",
    "# 等高線プロット\n",
    "plot_prediction(perceptron, w, b)\n",
    "\n",
    "# データ点をプロット\n",
    "plot_datapoint(x, t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 多層パーセプトロン(Multilayer perceptron)\n",
    "単純パーセプトロンはXORゲートが表現できませんでした。この問題は層の数を増やす(多層にする)ことで解決することができます。単純パーセプトロンを多層にしたものを多層パーセプトロン(Multilayer perceptron)と呼びます。\n",
    "\n",
    "グラフとして表すと、次の図のようになります。\n",
    "<img src=\"jupyterFigure/MLP_graph.png\" width=\"400\">\n",
    "\n",
    "数式で表すと、\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{z}^{(1)} &= \\sigma(\\mathbf{w}^{(1)}\\cdot \\mathbf{x} + b^{(1)}) \\\\\n",
    "y(\\mathbf{x}|\\mathbf{w}^{(1)}, b^{(1)},\\mathbf{w}^{(2)}, b^{(2)}) &= \\sigma(\\mathbf{w}^{(2)}\\cdot \\mathbf{z}^{(1)} + b^{(2)})\n",
    "\\end{align*}\n",
    "$$\n",
    "のようになります。単純パーセプトロンの出力ノードを、新たに入力として使ったような形になっています。\n",
    "\n",
    "活性化関数としてステップ関数を用いて、多層パーセプトロンがXORゲートを表現できることを示します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 例1: XORゲート\n",
    "NANDゲート・ORゲート・ANDゲートは単純パーセプトロンで構成できることを先程示しました。\n",
    "XORはNANDゲート・ORゲート・ANDゲートを組み合わせることで作ることができます。\n",
    "\n",
    "NANDゲートの出力を$z_1$、ORゲートの出力を$z_2$とし、$z_1$、$z_2$を入力として受け取るANDゲートを考えると、表のようになります。\n",
    "\n",
    "| $x_1$ | $x_2$ | $z_1$ | $z_2$ | $y$ |\n",
    "| -- | -- | -- | -- | -- |\n",
    "| 0 | 0 | 1 | 0 | 0 |\n",
    "| 0 | 1 | 1 | 1 | 1 |\n",
    "| 1 | 0 | 1 | 1 | 1 |\n",
    "| 1 | 1 | 0 | 1 | 0 |\n",
    "\n",
    "一例ですが、数式で明示的に書くと、\n",
    "$$\n",
    "\\begin{align*}\n",
    "z_1(x_1, x_2) &= \\Gamma(-(x_1 + x_2 - 1.5)) \\ \\text{(NANDゲート)} \\\\\n",
    "z_2(x_1, x_2) &= \\Gamma(x_1 + x_2 - 0.5) \\ \\text{(ORゲート)} \\\\\n",
    "y(z_1, z_2) &= \\Gamma(z_1 + z_2 - 1.5) \\ \\text{(ANDゲート)}\n",
    "\\end{align*}\n",
    "$$\n",
    "のようにしてXORゲートを構成することができます。\n",
    "\n",
    "この構成を視覚的に表したのが以下の図です。\n",
    "\n",
    "|ORゲート|NANDゲート|ANDゲート|XORゲート|\n",
    "|:---:|:---:|:---:|:---:|\n",
    "|![](jupyterFigure/OR.png)|![](jupyterFigure/NAND.png)|![](jupyterFigure/AND.png)|![](jupyterFigure/XOR.png)||\n",
    "\n",
    "１層のパーセプトロンでは、直線で領域分割することしかできませんでしたが、複数のパーセプトロンを組み合わせることでより複雑な領域の指定が可能になっていることがわかります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def xor_gate(x1, x2):\n",
    "    # OR Gate\n",
    "    a1 = or_gate(x1, x2)\n",
    "\n",
    "    # NAND Gate\n",
    "    a2 = nand_gate(x1, x2)\n",
    "\n",
    "    # AND Gate\n",
    "    y = and_gate(a1, a2)\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "# 正しく実装できているかを出力\n",
    "print(\"XOR Gate:\")\n",
    "print(\"  x1, x2, y\")\n",
    "print(\"   0,  0, \", xor_gate(0, 0), \"Correct\" if xor_gate(0, 0) == 0 else \"Wrong\")  # 正解は 0\n",
    "print(\"   0,  1, \", xor_gate(0, 1), \"Correct\" if xor_gate(0, 1) == 1 else \"Wrong\")  # 正解は 1\n",
    "print(\"   1,  0, \", xor_gate(1, 0), \"Correct\" if xor_gate(1, 0) == 1 else \"Wrong\")  # 正解は 1\n",
    "print(\"   1,  1, \", xor_gate(1, 1), \"Correct\" if xor_gate(1, 1) == 0 else \"Wrong\")  # 正解は 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 例2: 二次元ガウシアン\n",
    "単純パーセプトロンでは分類できなかった以下のようなデータセット\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ点の取得\n",
    "x, t = get_dataset_2()\n",
    "\n",
    "# データ点をプロット\n",
    "plot_datapoint(x, t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "を多層パーセプトロンで分類してみましょう。\n",
    "\n",
    "数式としては、\n",
    "$$\n",
    "\\begin{align*}\n",
    "a_j^{(1)} &= w_{ij}^{(1)}\\cdot x_{i} + b^{(1)}\\\\\n",
    "z_j^{(1)} &= \\sigma(a_j^{(1)}) \\\\\n",
    "a_j^{(2)} &= w_{ij}^{(2)}\\cdot x_{i} + b^{(2)}\\\\\n",
    "y         &= \\sigma(a^{(2)})\n",
    "\\end{align*}\n",
    "$$\n",
    "となります。\n",
    "\n",
    "<img src=\"jupyterFigure/MLP_graph.png\" width=\"400\">\n",
    "\n",
    "多層パーセプトロンでは、中間層のノードの数は自由に変えることができます。ここではノード数を3としてどのような予測がされるかを見てみましょう。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ点の取得\n",
    "x, t = get_dataset_2()\n",
    "\n",
    "# 多層パーセプトロンの定義\n",
    "def multilayerPerceptron(x, w1, b1, w2, b2):\n",
    "    # 1層目\n",
    "    a1 = np.dot(x, w1) + b1\n",
    "    z1 = sigmoid(a1)\n",
    "\n",
    "    # 2層目\n",
    "    a2 = np.dot(z1, w2) + b2\n",
    "    y = sigmoid(a2)\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "# パーセプトロンのパラメータ (正規分布で初期化)\n",
    "w1 = np.random.randn(2, 3)  # 入力ノード数=2, 出力ノード数=3\n",
    "b1 = np.random.randn(3)  # 出力ノード数=3\n",
    "w2 = np.random.randn(3, 1)  # 入力ノード数=3, 出力ノード数=1\n",
    "b2 = np.random.randn(1)  # 出力ノード数=1\n",
    "\n",
    "# 多層パーセプトロンの出力をプロット\n",
    "plot_prediction(multilayerPerceptron, w1, b1, w2, b2)\n",
    "\n",
    "## データ点をプロット\n",
    "plot_datapoint(x, t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "中間層のノードの数や、パラメータの値を変えてみて、どのように予測が変化するか見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ点の取得\n",
    "x, t = get_dataset_2()\n",
    "\n",
    "# 多層パーセプトロンの定義\n",
    "def multilayerPerceptron(x, w1, b1, w2, b2):\n",
    "    # 1層目\n",
    "    a1 = np.dot(x, w1) + b1\n",
    "    z1 = sigmoid(a1)\n",
    "\n",
    "    # 2層目\n",
    "    a2 = np.dot(z1, w2) + b2\n",
    "    y = sigmoid(a2)\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "# パーセプトロンのパラメータ\n",
    "num_z = 3  # 中間層のノード数\n",
    "w1 = np.zeros((2, num_z))  # 入力ノード数=2, 出力ノード数=num_z\n",
    "b1 = np.zeros(num_z)  # 出力ノード数=num_z\n",
    "w2 = np.zeros((num_z, 1))  # 入力ノード数=num_z, 出力ノード数=num_1\n",
    "b2 = np.zeros(1)  # 出力ノード数=1\n",
    "\n",
    "# FIXME: モデルパラメータの値を適当に変更してみましょう\n",
    "# 1層目のモデルパラメータ\n",
    "## z_0^{(1)}に関するパラメータ\n",
    "w1[0][0] = -1.0  # 1層目の0番目のノードと2層目の0番目のノードをつなぐ重みを変更\n",
    "w1[1][0] = +0.0  # 1層目の1番目のノードと2層目の0番目のノードをつなぐ重みを変更\n",
    "b1   [0] = +3.0  # 2層目の0番目のノードのバイアスを変更\n",
    "\n",
    "## z_1^{(1)}に関するパラメータ\n",
    "w1[0][1] = +0.0  # 1層目の0番目のノードと2層目の1番目のノードをつなぐ重みを変更\n",
    "w1[1][1] = +1.0  # 1層目の1番目のノードと2層目の1番目のノードをつなぐ重みを変更\n",
    "b1   [1] = +2.0  # 2層目の1番目のノードのバイアスを変更\n",
    "\n",
    "## z_2^{(1)}に関するパラメータ\n",
    "w1[0][2] = +1.0  # 1層目の0番目のノードと2層目の2番目のノードをつなぐ重みを変更\n",
    "w1[1][2] = +1.0  # 1層目の1番目のノードと2層目の2番目のノードをつなぐ重みを変更\n",
    "b1   [2] = +2.0  # 2層目の2番目のノードのバイアスを変更\n",
    "\n",
    "# 2層目のモデルパラメータ\n",
    "## z_i^{(1)}から出力(y)を作るパラメータ\n",
    "w2[0][0] = +1.0  # 2層目の0番目のノードと3層目の0番目のノードをつなぐ重みを変更\n",
    "w2[1][0] = +1.0  # 2層目の1番目のノードと3層目の0番目のノードをつなぐ重みを変更\n",
    "w2[2][0] = +1.0  # 2層目の2番目のノードと3層目の0番目のノードをつなぐ重みを変更\n",
    "b2   [0] = -3.0  # 3層目の0番目のノードのバイアスを変更\n",
    "\n",
    "# プロット\n",
    "plot_prediction(multilayerPerceptron, w1, b1, w2, b2)\n",
    "\n",
    "## データ点をプロット\n",
    "plot_datapoint(x, t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多層パーセプトロンの学習 (誤差逆伝搬)\n",
    "先程の単純パーセプトロンでは、誤差関数の一次微分を手計算で求めました。\n",
    "層の数を増やした多層パーセプトロンでは、一次微分を手計算するのは困難になります。\n",
    "微分の計算を効率よく行える手法として誤差逆伝搬法があります。\n",
    "ここでは、中間層が2層の多層パーセプトロンを例に、その手法について解説します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Test](jupyterFigure/MLP_3layer_graph.png \"Tmp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここでは簡単のためバイアスの項を省略します(バイアスが入った場合についても簡単に拡張できます)。\n",
    "ネットワークを表す式は、\n",
    "$$\n",
    "\\begin{align*}\n",
    "a_j^{(1)} &= w_{ij}^{(1)}\\cdot x_{i}\\\\\n",
    "z_j^{(1)} &= \\sigma(a_j^{(1)}) \\\\\n",
    "a_j^{(2)} &= w_{ij}^{(2)}\\cdot z_{i}^{(1)}\\\\\n",
    "z_j^{(2)} &= \\sigma(a_j^{(2)}) \\\\\n",
    "a_j^{(3)} &= w_{ij}^{(3)}\\cdot z_{i}^{(2)}\\\\\n",
    "y           &= \\sigma(a^{(3)})\n",
    "\\end{align*}\n",
    "$$\n",
    "となります。\n",
    "\n",
    "求めたいのは、誤差関数に対する各パラメータの一次微分です。すなわち\n",
    "$\\frac{\\partial E}{\\partial w_{ij}^{(1)}}$、$\\frac{\\partial E}{\\partial w_{ij}^{(2)}}$、$\\frac{\\partial E}{\\partial w_{ij}^{(3)}}$となります。\n",
    "\n",
    "これらは連鎖率を使って求めることができます。\n",
    "誤差関数は\n",
    "$$\n",
    "E(\\mathbf{w}) = \\frac{1}{N} \\sum_{n=1}^{N} E_n(\\mathbf{w}) = -\\frac{1}{N} \\sum_{n=1}^{N} \\left( t_n \\log y_n + \\left( 1-t_n \\right) \\log \\left( 1-y_n \\right) \\right)\n",
    "$$\n",
    "でした。\n",
    "また、\n",
    "$$\n",
    "\\delta_{i}^{(k)} = \\frac{\\partial E_n}{\\partial a_{i}^{(k)}}\n",
    "$$\n",
    "として定義して、それぞれ計算をすると、"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial E_n}{\\partial w_{ij}^{(3)}}\n",
    "&= \\frac{\\partial E_n}{\\partial  a_1^{(3)}}\\cdot \\frac{\\partial  a_1^{(3)}}{\\partial w_{ij}^{(3)}} \\\\\n",
    "&= \\left(y_n-t_n\\right) \\cdot z_1^{(2)} = \\delta_{1}^{(3)} \\cdot z_1^{(2)}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial E_n}{\\partial w_{ij}^{(2)}}\n",
    "&= \\frac{\\partial E_n}{\\partial  a_j^{(2)}}\\cdot \\frac{\\partial  a_j^{(2)}}{\\partial w_{ij}^{(2)}} \\\\\n",
    "&= \\left[ \\frac{\\partial E_n}{\\partial  a_1^{(3)}}\\cdot \\frac{\\partial a_1^{(3)}}{\\partial  z_j^{(2)}}\\cdot \\frac{\\partial z_j^{(2)}}{\\partial  a_j^{(2)}}\\right] \\cdot \\frac{\\partial  a_j^{(2)}}{\\partial w_{ij}^{(2)}} \\\\\n",
    "&= \\left[ \\delta_{1}^{(3)}\n",
    "\\cdot w_{j1}^{(3)} \n",
    "\\cdot \\sigma^{'}(a_j^{(2)}) \\right]\n",
    "\\cdot z_{i}^{(1)}  = \\delta_{j}^{(2)} \\cdot z_{i}^{(1)}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial E_n}{\\partial w_{ij}^{(1)}}\n",
    "&= \\frac{\\partial E_n}{\\partial  a_j^{(1)}}\\cdot \\frac{\\partial  a_j^{(1)}}{\\partial w_{ij}^{(1)}} \\\\\n",
    "&= \\left[ \\sum_k \\frac{\\partial E_n}{\\partial  a_{k}^{(2)}}\\cdot \\frac{\\partial a_{k}^{(2)}}{\\partial  z_j^{(1)}}\\cdot \\frac{\\partial z_j^{(1)}}{\\partial  a_j^{(1)}}\\right] \\cdot \\frac{\\partial  a_j^{(1)}}{\\partial w_{ij}^{(1)}} \\\\\n",
    "&= \\left[ \\sum_k \\delta_{k}^{(2)}\n",
    "\\cdot w_{jk}^{(2)} \n",
    "\\cdot \\sigma^{'}(a_j^{(1)}) \\right]\n",
    "\\cdot x_{i}  = \\delta_{j}^{(1)} \\cdot x_{i}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここで、入力に近い方にある重みの計算では、それよりも後にある重みの計算に使用した値が再利用することができることがわかります。\n",
    "このことを利用すると、複雑なネットワークの微分の計算がシステマティックに行えることになります。\n",
    "![Test](jupyterFigure/MLP_3layer_bp.png \"Tmp\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実際に計算する際には、下流側から順に$\\delta_{i}^{(k)}$を求めていきます。\n",
    "$\\delta_{i}^{(k)}$は活性化関数(ここではシグモイド関数)の微分値に、下流の誤差($\\delta_{j}^{(k+1)}$)の線形和をとったものとなります。\n",
    "$$\n",
    "\\delta_{i}^{(k)} = \n",
    "\\sigma^{'}(a_i^{(k)}) \\left(\n",
    "\\sum_j w_{ij}^{(k+1)} \\cdot \\delta_{j}^{(k+1)} \\right)\n",
    "$$\n",
    "重み($w_{ij}^{(k)}$)の微分はつながっているノードの$\\delta_{i}^{(k)}$と$z_{i}^{(k)}$を用いて、\n",
    "$$\n",
    "\\frac{\\partial E_n}{\\partial w_{ij}^{(k)}}  = \\delta_{j}^{(k)} \\cdot z_{i}^{(k-1)}\n",
    "$$\n",
    "として求めることができます。\n",
    "\n",
    "このようにして微分を求めることを誤差逆伝搬(Back-propagation)と呼びます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この誤差逆伝搬の式を先程の中間層1層の多層パーセプトロンに適用してみましょう。\n",
    "<img src=\"jupyterFigure/MLP_1layer_bp.png\" width=\"400\">\n",
    "次のような手順で一次微分を求めることができます。\n",
    "\n",
    "1. 入力($x_i$)とパラメータ($w_{ij}^{(1)},b_j^{(1)},w_{ij}^{(2)},b_j^{(2)}$)を元に順伝搬させて予測値$y$を得る。\n",
    "    $$\n",
    "    \\begin{align*}\n",
    "    a_j^{(1)} &= w_{ij}^{(1)}\\cdot x_{i} + b^{(1)}\\\\\n",
    "    z_j^{(1)} &= \\sigma(a_j^{(1)}) \\\\\n",
    "    a_j^{(2)} &= w_{ij}^{(2)}\\cdot x_{i} + b^{(2)}\\\\\n",
    "    y         &= \\sigma(a^{(2)})\n",
    "    \\end{align*} $$\n",
    "2. 出力$y$と真のラベル$t$を用いて誤差$\\delta$を伝搬させる。\n",
    "    $$\n",
    "    \\begin{align*}\n",
    "    \\delta_{1}^{(2)} &= \\left(y_n-t_n\\right) \\\\\n",
    "    \\delta_{j}^{(1)} &= \\sigma^{'}(a_j^{(1)}) \\cdot w_{j1}^{(2)}  \\cdot  \\delta_{1}^{(2)} \n",
    "    \\end{align*} $$\n",
    "3. 誤差$\\delta$を元に微分を計算\n",
    "    $$\n",
    "    \\begin{align*}\n",
    "    \\frac{\\partial E_n}{\\partial w_{ij}^{(2)}} &= \\delta_{j}^{(2)} \\cdot z_{i}^{(1)} \\\\\n",
    "    \\frac{\\partial E_n}{\\partial b_{j}^{(2)}}  &= \\delta_{j}^{(2)} \\\\\n",
    "    \\frac{\\partial E_n}{\\partial w_{ij}^{(1)}} &= \\delta_{j}^{(1)} \\cdot x_{i} \\\\\n",
    "    \\frac{\\partial E_n}{\\partial b_{j}^{(1)}}  &= \\delta_{j}^{(1)} \\\\\n",
    "    \\end{align*} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "誤差逆伝搬法をpythonで実装したものが以下になります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# シグモイド関数の微分\n",
    "def grad_sigmoid(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "\n",
    "# データ点の取得\n",
    "x, t = get_dataset_2()\n",
    "\n",
    "# パーセプトロンのパラメータ\n",
    "w1 = np.random.randn(2, 3)  # 入力ノード数=2, 出力ノード数=3\n",
    "b1 = np.random.randn(3)  # 出力ノード数=3\n",
    "w2 = np.random.randn(3, 1)  # 入力ノード数=3, 出力ノード数=1\n",
    "b2 = np.random.randn(1)  # 出力ノード数=1\n",
    "\n",
    "# 最急降下法での学習\n",
    "learning_rate = 1.0  # ステップ幅\n",
    "num_steps = 2000  # 繰り返し回数\n",
    "for _ in range(num_steps):\n",
    "    # 順伝搬させる\n",
    "    a1 = np.dot(x, w1) + b1\n",
    "    z1 = sigmoid(a1)\n",
    "    a2 = np.dot(z1, w2) + b2\n",
    "    y = sigmoid(a2)  # パーセプトロンの出力\n",
    "\n",
    "    # 一次微分を求めるために逆伝搬させる\n",
    "    grad_a2 = y - t\n",
    "    grad_w2 = np.einsum('ij,ik->ijk', z1, grad_a2)  # grad_w2 = z1 * grad_a2\n",
    "    grad_b2 = 1. * grad_a2\n",
    "\n",
    "    grad_a1 = grad_sigmoid(a1) * (grad_a2 * w2.T)\n",
    "    grad_w1 = np.einsum('ij,ik->ijk', x, grad_a1)  # grad_w1 = x * grad_a1\n",
    "    grad_b1 = 1. * grad_a1\n",
    "\n",
    "    # パラメータの更新 (mean(axis=0): 全てのデータ点に対しての平均値を使う)\n",
    "    w2 = w2 - learning_rate * grad_w2.mean(axis=0)\n",
    "    b2 = b2 - learning_rate * grad_b2.mean(axis=0)\n",
    "    w1 = w1 - learning_rate * grad_w1.mean(axis=0)\n",
    "    b1 = b1 - learning_rate * grad_b1.mean(axis=0)\n",
    "\n",
    "# プロット\n",
    "## パーセプトロンの出力を等高線プロット\n",
    "plot_prediction(multilayerPerceptron, w1, b1, w2, b2)\n",
    "\n",
    "## データ点をプロット\n",
    "plot_datapoint(x, t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "単純パーセプトロンでは、ある直線に対してどの程度離れているか、という指標でしか分類が行えていませんでしたが、多層パーセプトロンでは複数の直線からの距離を元に推定が行われています。\n",
    "\n",
    "上の学習で得られたパラメータを使って、実際に中間層でどのような分類が行われているのかを見てみます。\n",
    "図の$z_{i}^{(1)}$に対応するノードの出力をプロットします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パーセプトロンの出力を等高線プロット\n",
    "# 重みが(w1, b1)の1層パーセプトロンの出力を描画\n",
    "plot_prediction_line(perceptron, w1[:, 0], b1[0])  # 中間層の0番目のノードの出力\n",
    "plot_prediction_line(perceptron, w1[:, 1], b1[1])  # 中間層の1番目のノードの出力\n",
    "plot_prediction_line(perceptron, w1[:, 2], b1[2])  # 中間層の2番目のノードの出力\n",
    "\n",
    "# データ点をプロット\n",
    "plot_datapoint(x, t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このように、複数の(ここでは3つの)直線での分類を組み合わせて、2次元円形の領域を抜き出しています。\n",
    "隠れ層の存在により、複雑な関数を近似できることがわかりました。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 万能近似定理\n",
    "ここまでは分類問題を考えてきました。ここでは回帰問題を考えて、多層パーセプトロンの表現能力を確認します。\n",
    "\n",
    "分類問題では、パーセプトロンの出力を確率とみなすため、0~1の範囲を取るように、シグモイド関数を使いました。\n",
    "回帰問題では任意の範囲の出力を取らせるようにします。よく使うのは、最後の活性化関数として線形関数(すなわち何もしない)を使うことです。\n",
    "式として表すと、\n",
    "$$\n",
    "\\begin{align*}\n",
    "a_j^{(1)} &= w_{j}^{(1)}\\cdot x + b^{(1)}\\\\\n",
    "z_j^{(1)} &= \\sigma(a_j^{(1)}) \\\\\n",
    "a^{(2)} &= w_{i}^{(2)}\\cdot z_{i} + b^{(2)}\\\\\n",
    "y         &= a^{(2)}\n",
    "\\end{align*}\n",
    "$$\n",
    "となります。最後の行がシグモイド関数から、恒等写像に変わっています。グラフとして表すと、\n",
    "<img src=\"jupyterFigure/MLP_regression_graph.png\" width=\"400\">\n",
    "のようになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パーセプトロンの定義 (回帰)\n",
    "def multilayerPerceptronRegression(x, w1, b1, w2, b2):\n",
    "    a1 = np.dot(x, w1) + b1\n",
    "    z1 = sigmoid(a1)\n",
    "    a2 = np.dot(z1, w2) + b2\n",
    "    y = a2  # linear activation\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "入力として1次元の関数を使ってその表現能力を見てみます"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "誤差関数としては予測値と真の値との差の2乗和(二乗和誤差(mean squared error))を使います。すなわち、パラメータが観測値を説明する確率は\n",
    "$$\n",
    "p(\\mathbf{t}|\\mathbf{w},b ) = \\prod_{n=1}^{N} \\frac{1}{\\sqrt{2\\pi}\\sigma} e^{-\\frac{(y_n-t_n)^2}{2\\sigma^2}}\n",
    "$$\n",
    "であり、最小化する誤差関数は\n",
    "$$\n",
    "E(\\mathbf{w}, b) = \\frac{1}{N} \\sum_{n=1}^{N} E_n(\\mathbf{w}, b) = \\frac{1}{N} \\sum_{n=1}^{N} \\frac{1}{2} \\left( y_n - t_n \\right)^2\n",
    "$$\n",
    "誤差関数の微分は\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial E_n}{\\partial  a_1^{(k)}} \n",
    "&= \\frac{\\partial E_n}{\\partial  y} \\frac{\\partial y}{\\partial  a_1^{(k)}} \\\\\n",
    "&= \\left. y_n-t_n \\right.\n",
    "\\end{align*}\n",
    "$$\n",
    "となり、シグモイド関数+クロスエントロピーのときと似たような式になります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二次曲線"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sample = 20\n",
    "\n",
    "# x=(-1, 1), t=x^2\n",
    "x = np.linspace(-1, 1, num_sample)[:, np.newaxis]\n",
    "t = x * x\n",
    "\n",
    "# パーセプトロンのパラメータ\n",
    "num_nodes = 2  # 中間層のノード数\n",
    "w1 = np.random.randn(1, num_nodes)  # 入力ノード数=1, 出力ノード数=num_nodes\n",
    "b1 = np.random.randn(num_nodes)  # 出力ノード数=num_nodes\n",
    "w2 = np.random.randn(num_nodes, 1)  # 入力ノード数=num_nodes, 出力ノード数=1\n",
    "b2 = np.random.randn(1)  # 出力ノード数=1\n",
    "\n",
    "# うまく最適化されないときは以下で初期化してください\n",
    "# w1 = np.array([[+2.79, -2.791]])\n",
    "# b1 = np.array([[-2.80, -2.801]])\n",
    "# w2 = np.array([[2.55], [2.551]])\n",
    "# b2 = np.array([[-0.29]])\n",
    "\n",
    "# 最急降下法での学習\n",
    "learning_rate = 1.0  # ステップ幅\n",
    "num_steps = 10000  # 繰り返し回数\n",
    "for _ in range(num_steps):\n",
    "    # 順伝搬させる\n",
    "    a1 = np.dot(x, w1) + b1\n",
    "    z1 = sigmoid(a1)\n",
    "    a2 = np.dot(z1, w2) + b2\n",
    "    y = a2  # パーセプトロンの出力\n",
    "\n",
    "    # 一次微分を求めるために逆伝搬させる\n",
    "    grad_a2 = y - t\n",
    "    grad_w2 = np.einsum('ij,ik->ijk', z1, grad_a2)  # grad_w2 = z1 * grad_a2\n",
    "    grad_b2 = 1. * grad_a2\n",
    "\n",
    "    grad_a1 = grad_sigmoid(a1) * (grad_a2 * w2.T)\n",
    "    grad_w1 = np.einsum('ij,ik->ijk', x, grad_a1)  # grad_w1 = x * grad_a1\n",
    "    grad_b1 = 1. * grad_a1\n",
    "\n",
    "    # パラメータの更新 (mean(axis=0): 全てのデータ点に対しての平均値を使う)\n",
    "    w2 = w2 - learning_rate * grad_w2.mean(axis=0)\n",
    "    b2 = b2 - learning_rate * grad_b2.mean(axis=0)\n",
    "    w1 = w1 - learning_rate * grad_w1.mean(axis=0)\n",
    "    b1 = b1 - learning_rate * grad_b1.mean(axis=0)\n",
    "\n",
    "\n",
    "# プロット\n",
    "plot_prediction_regression(x, t, multilayerPerceptronRegression, w1, b1, w2, b2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "黒点がデータ点、赤線が多層パーセプトロンによる予測値を表しています。\n",
    "点線はパーセプトロンの中間層の各ノードにおける出力を表していて、点線の和が最後の出力(赤線)となります。\n",
    "\n",
    "二次関数($y=x^2$)は中間層が2層の多層パーセプトロンでおおよそ近似できていることがわかります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sin(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_sample = 20\n",
    "\n",
    "# x=(-pi, pi), t=sin(x)\n",
    "x = np.linspace(-np.pi, np.pi, num_sample)[:, np.newaxis]\n",
    "t = np.sin(x)\n",
    "\n",
    "# パーセプトロンのパラメータ\n",
    "num_nodes = 4  # 中間層のノード数\n",
    "w1 = np.random.randn(1, num_nodes)  # 入力ノード数=1, 出力ノード数=num_nodes\n",
    "b1 = np.random.randn(num_nodes)  # 出力ノード数=num_nodes\n",
    "w2 = np.random.randn(num_nodes, 1)  # 入力ノード数=num_nodes, 出力ノード数=1\n",
    "b2 = np.random.randn(1)  # 出力ノード数=1\n",
    "\n",
    "# うまく最適化されないときは以下で初期化してください\n",
    "# w1 = np.array([[+1.4, -1.4, 1.6, -1.8]])\n",
    "# b1 = np.array([[-4.2, -4.2, -0.8, -1.2]])\n",
    "# w2 = np.array([[-3.0], [3.0], [2.0], [-1.3]])\n",
    "# b2 = np.array([[-0.3]])\n",
    "\n",
    "# 最急降下法での学習\n",
    "learning_rate = 1.0  # ステップ幅\n",
    "num_steps = 10000  # 繰り返し回数\n",
    "for _ in range(num_steps):\n",
    "    # 順伝搬させる\n",
    "    a1 = np.dot(x, w1) + b1\n",
    "    z1 = sigmoid(a1)\n",
    "    a2 = np.dot(z1, w2) + b2\n",
    "    y = a2  # パーセプトロンの出力\n",
    "\n",
    "    # 一次微分を求めるために逆伝搬させる\n",
    "    grad_a2 = y - t\n",
    "    grad_w2 = np.einsum('ij,ik->ijk', z1, grad_a2)  # grad_w2 = z1 * grad_a2\n",
    "    grad_b2 = 1. * grad_a2\n",
    "\n",
    "    grad_a1 = grad_sigmoid(a1) * (grad_a2 * w2.T)\n",
    "    grad_w1 = np.einsum('ij,ik->ijk', x, grad_a1)  # grad_w1 = x * grad_a1\n",
    "    grad_b1 = 1. * grad_a1\n",
    "\n",
    "    # パラメータの更新 (mean(axis=0): 全てのデータ点に対しての平均値を使う)\n",
    "    w2 = w2 - learning_rate * grad_w2.mean(axis=0)\n",
    "    b2 = b2 - learning_rate * grad_b2.mean(axis=0)\n",
    "    w1 = w1 - learning_rate * grad_w1.mean(axis=0)\n",
    "    b1 = b1 - learning_rate * grad_b1.mean(axis=0)\n",
    "\n",
    "# プロット\n",
    "plot_prediction_regression(x, t, multilayerPerceptronRegression, w1, b1, w2, b2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sinカーブも中間層のノード数4の多層パーセプトロンで近似することができました。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 矩形関数\n",
    "より一般的な関数もノードの数を増やすことで任意の精度で近似できることが知られています。\n",
    "ここでは直感的にそれを確かめてみます。\n",
    "\n",
    "多層パーセプトロンは以下のような関数を近似できます。\n",
    "$$\n",
    "\\begin{align*}\n",
    "y(x) &= \\begin{cases}\n",
    "    1 & (-0.1 < a < 0.1)\\\\\n",
    "    0 & (\\text{otherwise})\n",
    "  \\end{cases}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sample = 100\n",
    "\n",
    "x = np.linspace(-1, 1, num_sample).reshape(-1, 1)\n",
    "t = np.where(np.logical_and(-0.1 < x, x < 0.1), 1, 0)\n",
    "\n",
    "# パーセプトロンのパラメータ\n",
    "w1 = np.array([[200, 200]])\n",
    "b1 = np.array([-20, +20])\n",
    "w2 = np.array([[-1], [1]])\n",
    "b2 = np.array([0.])\n",
    "\n",
    "# プロット\n",
    "plot_prediction_regression(x, t, multilayerPerceptronRegression, w1, b1, w2, b2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この関数を複数用意することで任意の関数が近似できます。\n",
    "<img src=\"jupyterFigure/rectangularFunction.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras\n",
    "はじめに述べたように、最近はDeep learningが簡単に扱えるツールが多々あります。\n",
    "ここではKerasというパッケージを使ってこれまで行ってきた学習を行います。\n",
    "KerasはtensorflowやTheanoといったDeep learningパッケージを簡単に扱うようにするためのラッパーで、実際には裏ではTensorflow等のライブラリが動いています。\n",
    "\n",
    "Kerasの詳細な使い方については[公式ドキュメント](https://keras.io/ja/)も適宜参照してください。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多層パーセプトロンで2次元ガウシアン状のシグナルを分類する問題をKerasでも実装してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflowが使うCPUの数を制限します。(VMを使う場合)\n",
    "%env OMP_NUM_THREADS=1\n",
    "%env TF_NUM_INTEROP_THREADS=1\n",
    "%env TF_NUM_INTRAOP_THREADS=1\n",
    "\n",
    "from tensorflow.config import threading\n",
    "num_threads = 1\n",
    "threading.set_inter_op_parallelism_threads(num_threads)\n",
    "threading.set_intra_op_parallelism_threads(num_threads)\n",
    "\n",
    "#ライブラリのインポート\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "# データ点の取得\n",
    "x, t = get_dataset_2()\n",
    "\n",
    "# モデルの定義\n",
    "model = Sequential([\n",
    "    Dense(3, activation='sigmoid', input_dim=2),  # ノード数が3の層を追加。活性化関数はシグモイド関数。\n",
    "    Dense(1, activation='sigmoid')  # ノード数が1の層を追加。活性化関数はシグモイド関数。\n",
    "])\n",
    "\n",
    "#  誤差関数としてクロスエントロピーを指定。最適化手法は(確率的)勾配降下法\n",
    "model.compile(loss='binary_crossentropy', optimizer=SGD(learning_rate=1.0))\n",
    "\n",
    "#  トレーニング\n",
    "model.fit(\n",
    "    x=x,\n",
    "    y=t,\n",
    "    batch_size=len(x),  # バッチサイズ。一回のステップで全てのデータを使うようにする。\n",
    "    epochs=3000,  # 学習のステップ数\n",
    "    verbose=0,  # 1とするとステップ毎に誤差関数の値などが表示される\n",
    ")\n",
    "\n",
    "# プロット\n",
    "## パーセプトロンの出力を等高線プロット\n",
    "plot_prediction(model.predict)\n",
    "\n",
    "## データ点をプロット\n",
    "plot_datapoint(x, t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "パーセプトロンの実装が簡潔に行えていることがわかります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kerasではモデルの変更も簡単に行えます。\n",
    "下の例では、\n",
    "- 1層あたりのノード数の変更\n",
    "- レイヤー数の変更\n",
    "- 活性化関数の変更\n",
    "- 誤差関数の最適化手法の変更\n",
    "\n",
    "を行っています。\n",
    "\n",
    "Kerasは深層学習で有用と知られている多くのテクニックが既に実装され、簡単に使えるようになっています。\n",
    "例えば[誤差関数](https://keras.io/ja/losses/)、[最適化手法](https://keras.io/ja/optimizers/)、[活性化関数](https://keras.io/ja/activations/)など、使える関数が日本語でもまとめられています。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# データ点の取得\n",
    "x, t = get_dataset_2()\n",
    "\n",
    "# モデルの定義\n",
    "model = Sequential([\n",
    "    Dense(32, activation='relu', input_dim=2),  # ノード数が32の層を追加。活性化関数はrelu。\n",
    "    Dense(16, activation='relu'),  # ノード数が16の層を追加。活性化関数はrelu。\n",
    "    Dense(8, activation='relu'),  # ノード数が8の層を追加。活性化関数はrelu。\n",
    "    Dense(1, activation='sigmoid')  # ノード数が1の層を追加。活性化関数はシグモイド関数。\n",
    "])\n",
    "\n",
    "#  誤差関数としてクロスエントロピーを指定。最適化手法は\"adam\"\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "#  トレーニング\n",
    "model.fit(\n",
    "    x=x,\n",
    "    y=t,\n",
    "    batch_size=128,  # len(x), #  バッチサイズ。128個のデータセット毎にパラメータの更新を行う。\n",
    "    epochs=500,  # 学習のステップ数\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# プロット\n",
    "## パーセプトロンの出力を等高線プロット\n",
    "plot_prediction(model.predict)\n",
    "\n",
    "## データ点をプロット\n",
    "plot_datapoint(x, t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 最後に\n",
    "\n",
    "実習課題として以下の2つを用意しました。\n",
    "- パラメータ最適化手法\n",
    "- 活性化関数とパラメータの初期化\n",
    "\n",
    "それぞれ別のjupyter notebookを用意しています。もし興味があれば取り組んでください。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "f5ee84ca7cc839add57a1456079373a6ef1d5daac4f4be388eaa02049720b4e1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
