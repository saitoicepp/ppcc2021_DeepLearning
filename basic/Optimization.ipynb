{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ライブラリのインポート\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先程は$f(x_1,x_2)=x_1^2 + x_2^2$の最小化を行いましたが、以下の関数(Rosenbrock function)を最小化してみましょう。\n",
    "$$\n",
    "f(x_1,x_2)=(1-x_1)^2 + 100(x_2-x_1^2)^2\n",
    "$$\n",
    "この関数は$(x_1,x_2)=(1, 1)$で最小値を取ります。\n",
    "微分は\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial f}{\\partial x_1} &= -2(1-x_1) - 400(x_2-x_1^2)x_1 \\\\\n",
    "\\frac{\\partial f}{\\partial x_2} &= 200(x_2-x_1^2) \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "となります。これを最急降下法(GD)で最小化すると、"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 目的関数の等高線プロット\n",
    "x1, x2 = np.mgrid[-0.8:1.2:100j, -0.1:1.1:100j]  # x1 = (-5, 5), x2 = (-5, 5) の範囲で100点x100点のメッシュを作成\\\n",
    "y = (1 - x1) ** 2 + 100 * (x2 - x1 ** 2) ** 2  # Rosenbrock関数\n",
    "plt.contour(x1, x2, np.log(y), linestyles='dashed', levels=10)  # 見やすさのため、z軸をlog-scaleにしています\n",
    "\n",
    "x1_history = []\n",
    "x2_history = []\n",
    "\n",
    "x1 = -0.5  # 初期値\n",
    "x2 = 0.5  # 初期値\n",
    "x1_history.append(x1)\n",
    "x2_history.append(x2)\n",
    "\n",
    "# 最適化\n",
    "learning_rate = 0.005  # ステップ幅\n",
    "num_steps = 100  # 繰り返し回数\n",
    "for _ in range(num_steps):\n",
    "    # 勾配を手計算で求める\n",
    "    grad_x1 = -2 * (1 - x1) - 400 * (x2 - x1 * x1) * x1\n",
    "    grad_x2 = 200 * (x2 - x1 * x1)\n",
    "\n",
    "    # 最急降下法で値を更新\n",
    "    x1 = x1 - learning_rate * grad_x1\n",
    "    x2 = x2 - learning_rate * grad_x2\n",
    "\n",
    "    x1_history.append(x1)\n",
    "    x2_history.append(x2)\n",
    "\n",
    "# 更新値履歴のプロット\n",
    "plt.plot(x1_history, x2_history,\n",
    "         color='black',\n",
    "         marker='o', markersize=5, markerfacecolor='None', markeredgecolor='black')  # プロット\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.xlim([-0.8, 1.2])\n",
    "plt.ylim([-0.1, 1.1])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習率やステップ回数を変化させてどのように学習が変わるかを見てみてください。最小値までたどり着くためにはどの程度のステップ数が必要でしょうか？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最急降下法は\n",
    "\n",
    "$$\n",
    "\\mathbf{x}^{(k+1)} = \\mathbf{x}^{(k)} - \\epsilon \\cdot \\left. \\frac{\\partial f}{\\partial \\mathbf{x}} \\right|_{\\mathbf{x}=\\mathbf{x}^{(k)}}\n",
    "$$\n",
    "のように値を更新するアルゴリズムでした。このアルゴリズムでは、その場その場の傾きに従って降下を行うため、ジグザクのパターンで無駄な動きをすることがあります。また、変数ごとに微分の大きさが大きく異なる時、うまく最適化ができません。\n",
    "\n",
    "この問題を解決するためにさまざまな最適化手法が提案されています。最適化手法について調べて、実装し、上の関数(Rosenbrock function)を最適化してください。\n",
    "また、実際の深層学習モデルに適応したときは、どのような性能の違いが見られるでしょうか？\n",
    "\n",
    "[Kerasに実装されている最適化手法のドキュメント](https://keras.io/ja/optimizers/)に有名な最適化手法のリストと参考文献があります。\n",
    "\n",
    "他の最適化手法の一例としては\n",
    "- Momentum\n",
    "- AdaGrad\n",
    "- RMSprop\n",
    "- Adam\n",
    "\n",
    "等があります。この内、特にAdamが深層学習でよく使われています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "いくつかの最適化アルゴリズムの実装ができたら、他の関数系でも同様の振る舞いをするのか、それともそうではないのか検証してください。\n",
    "関数の例は例えば[Wikipediaの例](https://en.wikipedia.org/wiki/Test_functions_for_optimization)にあります。他にも実際のニューラルネットワークの学習ではどのような振る舞いとなるでしょうか。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips\n",
    "以下のセルは実習をするにあたって参考になるかもしれないTipsです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JAX\n",
    "発展的な内容ですが、JAXを使うと微分計算が自動で行えるので便利です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# Rosenbrock関数\n",
    "def func(x):\n",
    "    return (1 - x[0]) ** 2 + 100 * (x[1] - x[0] ** 2) ** 2\n",
    "\n",
    "# 目的関数の等高線プロット\n",
    "x1, x2 = np.mgrid[-0.8:1.2:100j, -0.1:1.1:100j]  # x1 = (-5, 5), x2 = (-5, 5) の範囲で100点x100点のメッシュを作成\\\n",
    "plt.contour(x1, x2, np.log(func([x1, x2])), linestyles='dashed', levels=10)  # 見やすさのため、z軸をlog-scaleにしています\n",
    "\n",
    "x1_history = []\n",
    "x2_history = []\n",
    "\n",
    "x1 = -0.5  # 初期値\n",
    "x2 = 0.5  # 初期値\n",
    "x1_history.append(x1)\n",
    "x2_history.append(x2)\n",
    "\n",
    "# 最適化\n",
    "learning_rate = 0.005  # ステップ幅\n",
    "num_steps = 100  # 繰り返し回数\n",
    "for _ in range(num_steps):\n",
    "    # JAXを使うと微分が簡単に計算できます。\n",
    "    grad_x1, grad_x2 = jax.grad(func)([x1, x2])\n",
    "\n",
    "    # 最急降下法で値を更新\n",
    "    x1 = x1 - learning_rate * float(grad_x1)\n",
    "    x2 = x2 - learning_rate * float(grad_x2)\n",
    "\n",
    "    x1_history.append(x1)\n",
    "    x2_history.append(x2)\n",
    "\n",
    "# 更新値履歴のプロット\n",
    "plt.plot(x1_history, x2_history,\n",
    "         color='black',\n",
    "         marker='o', markersize=5, markerfacecolor='None', markeredgecolor='black')  # プロット\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.xlim([-0.8, 1.2])\n",
    "plt.ylim([-0.1, 1.1])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras での実装例\n",
    "ニューラルネットワークを対象に最適化手法の検証を行う場合はnumpyだけではなく、Keras/TensorflowやPyTrochなどのライブラリを使用するのが良いです。以下はKerasを使った実装例です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflowが使うCPUの数を制限します。(VMを使う場合)\n",
    "%env OMP_NUM_THREADS=1\n",
    "%env TF_NUM_INTEROP_THREADS=1\n",
    "%env TF_NUM_INTRAOP_THREADS=1\n",
    "\n",
    "from tensorflow.config import threading\n",
    "num_threads = 1\n",
    "threading.set_inter_op_parallelism_threads(num_threads)\n",
    "threading.set_intra_op_parallelism_threads(num_threads)\n",
    "\n",
    "# Tensorflowのインポート\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rosenbrock関数\n",
    "def func(x):\n",
    "    return (1 - x[0]) ** 2 + 100 * (x[1] - x[0] ** 2) ** 2\n",
    "\n",
    "# 目的関数の等高線プロット\n",
    "x1, x2 = np.mgrid[-0.8:1.2:100j, -0.1:1.1:100j]  # x1 = (-5, 5), x2 = (-5, 5) の範囲で100点x100点のメッシュを作成\\\n",
    "plt.contour(x1, x2, np.log(func([x1, x2])), linestyles='dashed', levels=10)  # 見やすさのため、z軸をlog-scaleにしています\n",
    "\n",
    "x1_history = []\n",
    "x2_history = []\n",
    "\n",
    "x1 = tf.Variable(-0.5)  # 初期値\n",
    "x2 = tf.Variable(0.5)  # 初期値\n",
    "x1_history.append(x1.numpy())\n",
    "x2_history.append(x2.numpy())\n",
    "\n",
    "# 最適化\n",
    "learning_rate = 0.005  # ステップ幅\n",
    "num_steps = 100  # 繰り返し回数\n",
    "for _ in range(num_steps):\n",
    "\n",
    "    # Tensorflowでも微分が簡単に計算できます。\n",
    "    with tf.GradientTape() as tape:\n",
    "        y = func([x1, x2])  # 順伝搬\n",
    "    grad_dx1, grad_dx2 = tape.gradient(y, [x1, x2])  # 逆伝搬\n",
    "\n",
    "    # 最急降下法で値を更新\n",
    "    x1.assign(x1 - learning_rate * grad_dx1)\n",
    "    x2.assign(x2 - learning_rate * grad_dx2)\n",
    "\n",
    "    x1_history.append(x1.numpy())\n",
    "    x2_history.append(x2.numpy())\n",
    "\n",
    "# 更新値履歴のプロット\n",
    "plt.plot(x1_history, x2_history,\n",
    "         color='black',\n",
    "         marker='o', markersize=5, markerfacecolor='None', markeredgecolor='black')  # プロット\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.xlim([-0.8, 1.2])\n",
    "plt.ylim([-0.1, 1.1])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KerasでのMLPの最適化 (Higgs Challenge datasetでの実装例)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datasetの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csvファイルの読み込み\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"/data/staff/deeplearning/atlas-higgs-challenge-2014-v2.csv\")\n",
    "\n",
    "# 全ての変数を入力に使います\n",
    "X = df[[\n",
    "    'DER_mass_MMC',\n",
    "    'DER_mass_transverse_met_lep',\n",
    "    'DER_mass_vis',\n",
    "    'DER_pt_h',\n",
    "    'DER_deltaeta_jet_jet',\n",
    "    'DER_mass_jet_jet',\n",
    "    'DER_prodeta_jet_jet',\n",
    "    'DER_deltar_tau_lep',\n",
    "    'DER_pt_tot',\n",
    "    'DER_sum_pt',\n",
    "    'DER_pt_ratio_lep_tau',\n",
    "    'DER_met_phi_centrality',\n",
    "    'DER_lep_eta_centrality',\n",
    "    'PRI_tau_pt',\n",
    "    'PRI_tau_eta',\n",
    "    'PRI_tau_phi',\n",
    "    'PRI_lep_pt',\n",
    "    'PRI_lep_eta',\n",
    "    'PRI_lep_phi',\n",
    "    'PRI_met',\n",
    "    'PRI_met_phi',\n",
    "    'PRI_met_sumet',\n",
    "    'PRI_jet_num',\n",
    "    'PRI_jet_leading_pt',\n",
    "    'PRI_jet_leading_eta',\n",
    "    'PRI_jet_leading_phi',\n",
    "    'PRI_jet_subleading_pt',\n",
    "    'PRI_jet_subleading_eta',\n",
    "    'PRI_jet_subleading_phi',\n",
    "    'PRI_jet_all_pt'\n",
    "]]\n",
    "\n",
    "# 目標変数とweightの指定\n",
    "Y = df['Label']\n",
    "W = df['KaggleWeight']\n",
    "\n",
    "# X を numpy.array 形式に変換します。\n",
    "X = X.values\n",
    "\n",
    "# Y を s/b から 1/0に変換します。\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "Y = LabelEncoder().fit_transform(Y)\n",
    "Y = Y[:, np.newaxis]\n",
    "\n",
    "# W を numpy.array 形式に変換します。\n",
    "W = W.values\n",
    "W = W[:, np.newaxis]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全データを使うと時間がかかるので、1000イベントだけを使用\n",
    "X = X[:1000]\n",
    "Y = Y[:1000]\n",
    "W = W[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデルの定義と学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの定義\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=X.shape[1:]),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# ロス関数は交差エントロピーを使用\n",
    "bce_loss = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "loss_history = []\n",
    "\n",
    "learning_rate = 0.001  # ステップ幅\n",
    "num_epochs = 100  # 繰り返し回数\n",
    "for ie in range(num_epochs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # 順伝搬\n",
    "        Y_pred = model(X, training=True)  # モデルの出力を得る\n",
    "        loss = bce_loss(Y, Y_pred, W)  # モデル出力を使ってロスの計算\n",
    "    gradients = tape.gradient(loss, model.trainable_weights)  # ロスの値に対する勾配を計算\n",
    "\n",
    "    for var, grad in zip(model.trainable_weights, gradients):\n",
    "        var.assign(var - learning_rate * grad)  # 最急降下法でパラメータを更新\n",
    "    \n",
    "    # ロスの値を出力・記録\n",
    "    print(f'epoch = {ie}, loss = {loss}')\n",
    "    loss_history += [loss]\n",
    "\n",
    "plt.plot(loss_history)\n",
    "# plt.yscale('log')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "f5ee84ca7cc839add57a1456079373a6ef1d5daac4f4be388eaa02049720b4e1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
