{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ライブラリのインポート\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPUによる高速化\n",
    "\n",
    "TensorflowやPyTorchといった深層学習ライブラリはGPUでの実行がサポートされています。\n",
    "GPUを使うことで大規模計算が高速化できることがあります。\n",
    "\n",
    "このノートブックではTensorflow/KerasやPyTorchをGPU上で実行し、CPUでの実行したときと計算速度を比較します。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPUの利用状況の確認\n",
    "\n",
    "現在接続しているノードのGPU使用率は以下のコマンドで確認できます。\n",
    "```bash\n",
    "$ nvidia-smi\n",
    "Thu Jul 20 11:29:18 2023       \n",
    "+-----------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 520.61.05    Driver Version: 520.61.05    CUDA Version: 11.8     |\n",
    "|-------------------------------+----------------------+----------------------+\n",
    "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                               |                      |               MIG M. |\n",
    "|===============================+======================+======================|\n",
    "|   0  NVIDIA A100-PCI...  On   | 00000000:1F:00.0 Off |                    0 |\n",
    "| N/A   84C    P0   129W / 250W |   3493MiB / 40960MiB |     78%      Default |\n",
    "|                               |                      |             Disabled |\n",
    "+-------------------------------+----------------------+----------------------+\n",
    "|   1  NVIDIA A100-PCI...  On   | 00000000:20:00.0 Off |                    0 |\n",
    "| N/A   84C    P0   175W / 250W |   2823MiB / 40960MiB |     79%      Default |\n",
    "|                               |                      |             Disabled |\n",
    "+-------------------------------+----------------------+----------------------+\n",
    "\n",
    "+-----------------------------------------------------------------------------+\n",
    "| Processes:                                                                  |\n",
    "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
    "|        ID   ID                                                   Usage      |\n",
    "|=============================================================================|\n",
    "|    0   N/A  N/A   3460006      C   python                           1242MiB |\n",
    "|    0   N/A  N/A   3461170      C   ...HostWorker.run_executable     1494MiB |\n",
    "|    1   N/A  N/A   3460006      C   python                            598MiB |\n",
    "|    1   N/A  N/A   3461170      C   ...HostWorker.run_executable     2216MiB |\n",
    "+-----------------------------------------------------------------------------+\n",
    "```\n",
    "このノードにはGPUが2台搭載されており、それぞれ2つのプロセスがGPUを使用しています。\n",
    "\n",
    "自分のプロセスがゾンビ状態となってGPUのメモリを専有してしまうことがないように定期的にチェックするのが好ましいです。\n",
    "\n",
    "GPUを使っているプロセスを起動したユーザーを確認するコマンドの一例は以下です。\n",
    "```bash\n",
    "$ ps -up `nvidia-smi --query-compute-apps=pid --format=csv,noheader | sort -u`\n",
    "USER         PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\n",
    "```\n",
    "ここで表示されるメモリの値はGPU上のメモリではないことに注意してください。\n",
    "\n",
    "notebook上では\"!\"をつけることでbashコマンドが実行できます。     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ps -up `nvidia-smi --query-compute-apps=pid --format=csv,noheader | sort -u`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用するGPUの指定\n",
    "まずは使うGPUを指定します。このステップなしでもGPUは利用できますが、GPUを専有してしまうことで共有マシンを使用している他のユーザーに迷惑をかけてしまうことがあります。\n",
    "今回の講習では、使うGPUは1つのみに限定させます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_available_gpu_index():\n",
    "    import random\n",
    "    import subprocess\n",
    "\n",
    "    # ノード上のGPUのインデックスとuuidを取得します。\n",
    "    p = subprocess.run(\n",
    "        f\"nvidia-smi --query-gpu=index,gpu_uuid --format=csv,noheader\",\n",
    "        shell=True,\n",
    "        stdout=subprocess.PIPE,\n",
    "    )\n",
    "    gpu_uuid_to_index = {}\n",
    "    for v in p.stdout.decode().split('\\n'):\n",
    "        if len(v) == 0:\n",
    "            continue\n",
    "        index, uuid = v.split(',')\n",
    "        gpu_uuid_to_index[uuid.strip()] = index.strip()\n",
    "\n",
    "    # GPUを使っているプロセスのuuidを取得します。\n",
    "    p = subprocess.run(\n",
    "        f\"nvidia-smi --query-compute-apps=gpu_uuid --format=csv,noheader\",\n",
    "        shell=True,\n",
    "        stdout=subprocess.PIPE,\n",
    "    )\n",
    "    ret = p.stdout.decode().split('\\n')\n",
    "    ret = set(ret)\n",
    "    for uuid in ret:\n",
    "        if len(uuid) == 0:\n",
    "            continue\n",
    "        gpu_uuid_to_index.pop(uuid)\n",
    "\n",
    "    # どのプロセスからも使用されていないGPUのインデックスをランダムに一つ返します。\n",
    "    # GPUが使用できない場合は空文字列を返します。\n",
    "    if len(gpu_uuid_to_index) == 0:\n",
    "        print('GPU is unavailable.')\n",
    "        return \"\"\n",
    "    else:\n",
    "        available_gpu_index = list(gpu_uuid_to_index.values())\n",
    "        return random.choice(available_gpu_index)\n",
    "\n",
    "# 使用されていないGPUを選択します。\n",
    "gpu_index = get_available_gpu_index()\n",
    "print(gpu_index)\n",
    "\n",
    "# 使うGPUを一つに制限します。\n",
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = gpu_index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorchが使うCPUの数を制限します。(VMを使う場合)\n",
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "os.environ['MKL_NUM_THREADS'] = '1'\n",
    "\n",
    "from torch import set_num_threads, set_num_interop_threads\n",
    "num_threads = 1\n",
    "set_num_threads(num_threads)\n",
    "set_num_interop_threads(num_threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU上でのPyTorchの計算\n",
    "PyTorchでは入力データ(tensor)やモデルをどのデバイスで計算させるか明示的に指定する必要があります。\n",
    "tensorやモデルオブジェクトに`.cuda()`、または`.to('cuda')`を指定することでGPUメモリにオブジェクトをコピーできます。\n",
    "\n",
    "#### tensor オブジェクトのGPUメモリへの移動\n",
    "```python\n",
    "from torch import tensor\n",
    "tensor(1)  # CPU\n",
    "tensor(1).cpu()  # CPU\n",
    "tensor(1).cuda()  # GPU\n",
    "tensor(1).to('cpu')  # CPU\n",
    "tensor(1).to('cuda')  # GPU\n",
    "```\n",
    "\n",
    "#### Model オブジェクトのGPUメモリへの移動\n",
    "```python\n",
    "from torch.nn import Sequential\n",
    "from torch.nn import Linear\n",
    "from torch.nn import Sigmoid\n",
    "from torch.nn import ReLU\n",
    "model = Sequential(\n",
    "    Linear(in_features=2, out_features=32),\n",
    "    Sigmoid(),\n",
    ")\n",
    "\n",
    "model  # CPU\n",
    "model.cpu()  # CPU\n",
    "model.cuda()  # GPU\n",
    "model.cpu()  # CPU\n",
    "model.to('cpu')  # CPU\n",
    "model.to('cuda')  # GPU\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLPのトレーニング\n",
    "\n",
    "MLPのトレーニングをCPU/GPUでそれぞれ実行することで、計算時間の変化を確認します。\n",
    "\n",
    "データセットは乱数で適当に作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import default_rng\n",
    "rng = default_rng(seed=0)\n",
    "\n",
    "# 10万行、入力次元10のランダムなデータを作成。\n",
    "datasize = 100000\n",
    "x = rng.normal(size=(datasize, 10))\n",
    "t = rng.integers(0, 2, size=(datasize, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch import tensor\n",
    "ds = TensorDataset(tensor(x).float(), tensor(t).float())\n",
    "dataloader = DataLoader(ds, batch_size=1024, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPUによるサンプルコード"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorchでは明示的に指定しない限り、CPU上で計算が行われます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Sequential\n",
    "from torch.nn import Linear\n",
    "from torch.nn import Sigmoid\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import BCELoss\n",
    "from torch.optim import Adam\n",
    "from torch import from_numpy\n",
    "from torch.cuda import synchronize\n",
    "import time\n",
    "\n",
    "# モデルの定義\n",
    "model = Sequential(\n",
    "    Linear(in_features=10, out_features=256),  # ノード数が256の層を追加。\n",
    "    ReLU(),  # 活性化関数はReLU。\n",
    "    Linear(in_features=256, out_features=256),  # ノード数が256の層を追加。\n",
    "    ReLU(),  # 活性化関数はReLU。\n",
    "    Linear(in_features=256, out_features=256),  # ノード数が256の層を追加。\n",
    "    ReLU(),  # 活性化関数はReLU。\n",
    "    Linear(in_features=256, out_features=256),  # ノード数が256の層を追加。\n",
    "    ReLU(),  # 活性化関数はReLU。\n",
    "    Linear(in_features=256, out_features=256),  # ノード数が256の層を追加。\n",
    "    ReLU(),  # 活性化関数はReLU。\n",
    "    Linear(in_features=256, out_features=1),  # ノード数が1の層を追加。\n",
    "    Sigmoid(),  # 活性化関数はシグモイド関数。\n",
    ")\n",
    "# 誤差関数としてクロスエントロピーを指定。最適化手法は(確率的)勾配降下法\n",
    "loss_fn = BCELoss()\n",
    "optimizer = Adam(model.parameters())\n",
    "\n",
    "# トレーニング\n",
    "for i_epoch in range(10):\n",
    "\n",
    "    # エポックごとのロス、accuracyを計算するための変数\n",
    "    loss_total = 0.\n",
    "\n",
    "    # 時間の測定\n",
    "    synchronize()\n",
    "    start_time = time.time()\n",
    "\n",
    "    for x, t in dataloader:\n",
    "        # 順伝搬\n",
    "        y_pred = model(x)\n",
    "\n",
    "        # ロスの計算\n",
    "        loss = loss_fn(y_pred, t)\n",
    "        loss_total += loss.detach().numpy()\n",
    "\n",
    "        # 誤差逆伝播の前に各パラメータの勾配の値を0にセットする。\n",
    "        # これをしないと、勾配の値はそれまでの値との和がとられる。\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 誤差逆伝播。各パラメータの勾配が計算される。\n",
    "        loss.backward()\n",
    "\n",
    "        # 各パラメータの勾配の値を基に、optimizerにより値が更新される。\n",
    "        optimizer.step()\n",
    "\n",
    "    # ロス、accuracyをミニバッチの数で割って平均を取ります。\n",
    "    loss_total /= len(dataloader)\n",
    "    synchronize()  # GPUの処理が終わるのを待ちます。\n",
    "    print(f'epoch = {i_epoch}, time = {time.time() - start_time: .3f} sec, loss = {loss_total}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPUによるサンプルコード\n",
    "tensorやmodelをGPU上に移動する(`.cuda()`の追加)ことで、GPUを使ったトレーニングができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Sequential\n",
    "from torch.nn import Linear\n",
    "from torch.nn import Sigmoid\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import BCELoss\n",
    "from torch.optim import Adam\n",
    "from torch import from_numpy\n",
    "from torch.cuda import synchronize\n",
    "import time\n",
    "\n",
    "# モデルの定義\n",
    "model = Sequential(\n",
    "    Linear(in_features=10, out_features=256),  # ノード数が256の層を追加。\n",
    "    ReLU(),  # 活性化関数はReLU。\n",
    "    Linear(in_features=256, out_features=256),  # ノード数が256の層を追加。\n",
    "    ReLU(),  # 活性化関数はReLU。\n",
    "    Linear(in_features=256, out_features=256),  # ノード数が256の層を追加。\n",
    "    ReLU(),  # 活性化関数はReLU。\n",
    "    Linear(in_features=256, out_features=256),  # ノード数が256の層を追加。\n",
    "    ReLU(),  # 活性化関数はReLU。\n",
    "    Linear(in_features=256, out_features=256),  # ノード数が256の層を追加。\n",
    "    ReLU(),  # 活性化関数はReLU。\n",
    "    Linear(in_features=256, out_features=1),  # ノード数が1の層を追加。\n",
    "    Sigmoid(),  # 活性化関数はシグモイド関数。\n",
    ")\n",
    "# 誤差関数としてクロスエントロピーを指定。最適化手法は(確率的)勾配降下法\n",
    "loss_fn = BCELoss()\n",
    "optimizer = Adam(model.parameters())\n",
    "\n",
    "# GPUメモリにモデルをコピーします。\n",
    "model = model.cuda()\n",
    "\n",
    "# トレーニング\n",
    "for i_epoch in range(10):\n",
    "\n",
    "    # エポックごとのロス、accuracyを計算するための変数\n",
    "    loss_total = 0.\n",
    "\n",
    "    # 時間の測定\n",
    "    synchronize()\n",
    "    start_time = time.time()\n",
    "\n",
    "    for x, t in dataloader:\n",
    "        # GPUメモリにデータをコピーします。\n",
    "        x = x.cuda()\n",
    "        t = t.cuda()\n",
    "\n",
    "        # 順伝搬\n",
    "        y_pred = model(x)\n",
    "\n",
    "        # ロスの計算\n",
    "        loss = loss_fn(y_pred, t)\n",
    "        loss_total += loss.detach().cpu().numpy()  # lossの合計値を記録するため、値をCPUにコピーします。\n",
    "\n",
    "        # 誤差逆伝播の前に各パラメータの勾配の値を0にセットする。\n",
    "        # これをしないと、勾配の値はそれまでの値との和がとられる。\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 誤差逆伝播。各パラメータの勾配が計算される。\n",
    "        loss.backward()\n",
    "\n",
    "        # 各パラメータの勾配の値を基に、optimizerにより値が更新される。\n",
    "        optimizer.step()\n",
    "\n",
    "    # ロス、accuracyをミニバッチの数で割って平均を取ります。\n",
    "    loss_total /= len(dataloader)\n",
    "    synchronize()  # GPUの処理が終わるのを待ちます。\n",
    "    print(f'epoch = {i_epoch}, time = {time.time() - start_time: .3f} sec, loss = {loss_total}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "計算時間はどのように変化したでしょうか。\n",
    "モデルサイズが変化すると、計算時間はどのように変わるでしょうか？"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
