{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ライブラリのインポート\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPUによる高速化\n",
    "\n",
    "TensorflowやPyTorchといった深層学習ライブラリはGPUでの実行がサポートされています。\n",
    "GPUを使うことで大規模計算が高速化できることがあります。\n",
    "\n",
    "このノートブックではTensorflow/KerasやPyTorchをGPU上で実行し、CPUでの実行したときと計算速度を比較します。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPUの利用状況の確認\n",
    "\n",
    "現在接続しているノードのGPU使用率は`nvidia-smi`コマンドで確認できます。\n",
    "```bash\n",
    "$ nvidia-smi\n",
    "Fri Jul 28 17:22:22 2023       \n",
    "+---------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 530.30.02              Driver Version: 530.30.02    CUDA Version: 12.1     |\n",
    "|-----------------------------------------+----------------------+----------------------+\n",
    "| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                      |               MIG M. |\n",
    "|=========================================+======================+======================|\n",
    "|   0  NVIDIA A100-SXM4-40GB           Off| 00000000:13:00.0 Off |                   On |\n",
    "| N/A   24C    P0               51W / 400W|    877MiB / 40960MiB |     N/A      Default |\n",
    "|                                         |                      |              Enabled |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "\n",
    "+---------------------------------------------------------------------------------------+\n",
    "| MIG devices:                                                                          |\n",
    "+------------------+--------------------------------+-----------+-----------------------+\n",
    "| GPU  GI  CI  MIG |                   Memory-Usage |        Vol|      Shared           |\n",
    "|      ID  ID  Dev |                     BAR1-Usage | SM     Unc| CE ENC DEC OFA JPG|\n",
    "|                  |                                |        ECC|                       |\n",
    "|==================+================================+===========+=======================|\n",
    "|  0    7   0   0  |              12MiB /  4864MiB  | 14      0 |  1   0    0    0    0 |\n",
    "|                  |               0MiB /  8191MiB  |           |                       |\n",
    "+------------------+--------------------------------+-----------+-----------------------+\n",
    "|  0    8   0   1  |              12MiB /  4864MiB  | 14      0 |  1   0    0    0    0 |\n",
    "|                  |               0MiB /  8191MiB  |           |                       |\n",
    "+------------------+--------------------------------+-----------+-----------------------+\n",
    "|  0    9   0   2  |             802MiB /  4864MiB  | 14      0 |  1   0    0    0    0 |\n",
    "|                  |               2MiB /  8191MiB  |           |                       |\n",
    "+------------------+--------------------------------+-----------+-----------------------+\n",
    "|  0   10   0   3  |              12MiB /  4864MiB  | 14      0 |  1   0    0    0    0 |\n",
    "|                  |               0MiB /  8191MiB  |           |                       |\n",
    "+------------------+--------------------------------+-----------+-----------------------+\n",
    "|  0   11   0   4  |              12MiB /  4864MiB  | 14      0 |  1   0    0    0    0 |\n",
    "|                  |               0MiB /  8191MiB  |           |                       |\n",
    "+------------------+--------------------------------+-----------+-----------------------+\n",
    "|  0   12   0   5  |              12MiB /  4864MiB  | 14      0 |  1   0    0    0    0 |\n",
    "|                  |               0MiB /  8191MiB  |           |                       |\n",
    "+------------------+--------------------------------+-----------+-----------------------+\n",
    "|  0   13   0   6  |              12MiB /  4864MiB  | 14      0 |  1   0    0    0    0 |\n",
    "|                  |               0MiB /  8191MiB  |           |                       |\n",
    "+------------------+--------------------------------+-----------+-----------------------+\n",
    "                                                                                         \n",
    "+---------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                            |\n",
    "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
    "|        ID   ID                                                             Usage      |\n",
    "|=======================================================================================|\n",
    "|    0    9    0      11117      C   .../local/venv/deeplearning/bin/python      782MiB |\n",
    "+---------------------------------------------------------------------------------------+\n",
    "```\n",
    "このノードではGPU1台が搭載されていますが、複数のユーザーで共有して使用できるようにGPUを7分割しています。\n",
    "１つ目のブロックから、NVIDIA A100というGPUが載っていて、メモリは40960MiB ということが読み取れます。\n",
    "2つ目のブロックでは7分割された各GPUの状態が表示されています。上から3つ目のデバイス(GIID=9, MIG Dev=2)ではGPUメモリが802MiB使用されています。\n",
    "最後のブロックは、GPUを使用しているプロセスが表示されています。GIID=9 のGPUを1つのプロセスが使用していることが読み取れます。\n",
    "\n",
    "自分のプロセスがゾンビ状態となってGPUのメモリを専有してしまうことがないように定期的にチェックしてください。\n",
    "\n",
    "GPUを使っているプロセスを起動したユーザーを確認するコマンドの一例は以下です。\n",
    "```bash\n",
    "$ ps -up `nvidia-smi --query-compute-apps=pid --format=csv,noheader | sort -u`\n",
    "USER         PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\n",
    "```\n",
    "ここで表示されるメモリの値はGPU上のメモリではないことに注意してください。\n",
    "\n",
    "notebook上では\"!\"をつけることでbashコマンドが実行できます。     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用するGPUの指定\n",
    "まずは使うGPUを指定します。このステップなしでもGPUは利用できますが、GPUを専有してしまうことで共有マシンを使用している他のユーザーに迷惑をかけてしまうことがあります。\n",
    "今回の講習では、使うGPUは1つのみに限定させます。\n",
    "\n",
    "GPUを指定するには、TensorflowやPyTorchをimportする前に環境変数`CUDA_VISIBLE_DEVICES`にGPUのUUIDをしてしてください。\n",
    "```python\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = 'MIG-554c4086-6ae7-5b25-8c39-e5980d23ae92'\n",
    "```\n",
    "UUIDは以下のようにして確認することができます。\n",
    "```bash\n",
    "[saito@centos7 ~]$ nvidia-smi -L\n",
    "GPU 0: NVIDIA A100-SXM4-40GB (UUID: GPU-07ce7acc-c626-f86c-3b8c-170a84e404b3)\n",
    "  MIG 1g.5gb      Device  0: (UUID: MIG-554c4086-6ae7-5b25-8c39-e5980d23ae92)\n",
    "  MIG 1g.5gb      Device  1: (UUID: MIG-9b9d3d09-b7d8-5351-af1e-5607f0fbd02a)\n",
    "  MIG 1g.5gb      Device  2: (UUID: MIG-0dfbf33f-6966-5b43-a167-2cb7be339805)\n",
    "  MIG 1g.5gb      Device  3: (UUID: MIG-1baae807-2fcd-5438-8a89-d320680ade08)\n",
    "  MIG 1g.5gb      Device  4: (UUID: MIG-cedbde95-8c79-51e0-b848-7bf73342a233)\n",
    "  MIG 1g.5gb      Device  5: (UUID: MIG-84f9461f-49b6-5343-a3b9-5be94986cdad)\n",
    "  MIG 1g.5gb      Device  6: (UUID: MIG-95045bde-aa51-5831-8983-63dbf0262e77)\n",
    "```\n",
    "Device ID(上の例だと0 ~ 6)と`nvidia-smi`の出力を見比べて、使用されて**いない**GPUを選択するようにしてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使うGPUを一つに制限します。\n",
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = 'MIG-554c4086-6ae7-5b25-8c39-e5980d23ae92'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorchが使うCPUの数を制限します。(VMを使う場合)\n",
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "os.environ['MKL_NUM_THREADS'] = '1'\n",
    "\n",
    "from torch import set_num_threads, set_num_interop_threads\n",
    "num_threads = 1\n",
    "set_num_threads(num_threads)\n",
    "set_num_interop_threads(num_threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU上でのPyTorchの計算\n",
    "PyTorchでは入力データ(tensor)やモデルをどのデバイスで計算させるか明示的に指定する必要があります。\n",
    "tensorやモデルオブジェクトに`.cuda()`、または`.to('cuda')`を指定することでGPUメモリにオブジェクトをコピーできます。\n",
    "\n",
    "#### tensor オブジェクトのGPUメモリへの移動\n",
    "```python\n",
    "from torch import tensor\n",
    "tensor(1)  # CPU\n",
    "tensor(1).cpu()  # CPU\n",
    "tensor(1).cuda()  # GPU\n",
    "tensor(1).to('cpu')  # CPU\n",
    "tensor(1).to('cuda')  # GPU\n",
    "```\n",
    "\n",
    "#### Model オブジェクトのGPUメモリへの移動\n",
    "```python\n",
    "from torch.nn import Sequential\n",
    "from torch.nn import Linear\n",
    "from torch.nn import Sigmoid\n",
    "from torch.nn import ReLU\n",
    "model = Sequential(\n",
    "    Linear(in_features=2, out_features=32),\n",
    "    Sigmoid(),\n",
    ")\n",
    "\n",
    "model  # CPU\n",
    "model.cpu()  # CPU\n",
    "model.cuda()  # GPU\n",
    "model.cpu()  # CPU\n",
    "model.to('cpu')  # CPU\n",
    "model.to('cuda')  # GPU\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLPのトレーニング\n",
    "\n",
    "MLPのトレーニングをCPU/GPUでそれぞれ実行することで、計算時間の変化を確認します。\n",
    "\n",
    "データセットは乱数で適当に作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import default_rng\n",
    "rng = default_rng(seed=0)\n",
    "\n",
    "# 100万行、入力次元100のランダムなデータを作成。\n",
    "datasize = 1000000\n",
    "x = rng.normal(size=(datasize, 100))\n",
    "t = rng.integers(0, 2, size=(datasize, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch import tensor\n",
    "ds = TensorDataset(tensor(x).float(), tensor(t).float())\n",
    "dataloader = DataLoader(ds, batch_size=1024, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPUによるサンプルコード"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorchでは明示的に指定しない限り、CPU上で計算が行われます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Sequential\n",
    "from torch.nn import Linear\n",
    "from torch.nn import Sigmoid\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import BCELoss\n",
    "from torch.optim import Adam\n",
    "from torch import from_numpy\n",
    "from torch.cuda import synchronize\n",
    "import time\n",
    "\n",
    "# モデルの定義\n",
    "model = Sequential(\n",
    "    Linear(in_features=100, out_features=256),  # ノード数が256の層を追加。\n",
    "    ReLU(),  # 活性化関数はReLU。\n",
    "    Linear(in_features=256, out_features=256),  # ノード数が256の層を追加。\n",
    "    ReLU(),  # 活性化関数はReLU。\n",
    "    Linear(in_features=256, out_features=256),  # ノード数が256の層を追加。\n",
    "    ReLU(),  # 活性化関数はReLU。\n",
    "    Linear(in_features=256, out_features=256),  # ノード数が256の層を追加。\n",
    "    ReLU(),  # 活性化関数はReLU。\n",
    "    Linear(in_features=256, out_features=256),  # ノード数が256の層を追加。\n",
    "    ReLU(),  # 活性化関数はReLU。\n",
    "    Linear(in_features=256, out_features=1),  # ノード数が1の層を追加。\n",
    "    Sigmoid(),  # 活性化関数はシグモイド関数。\n",
    ")\n",
    "# 誤差関数としてクロスエントロピーを指定。最適化手法は(確率的)勾配降下法\n",
    "loss_fn = BCELoss()\n",
    "optimizer = Adam(model.parameters())\n",
    "\n",
    "# トレーニング\n",
    "for i_epoch in range(5):\n",
    "\n",
    "    # エポックごとのロス、accuracyを計算するための変数\n",
    "    loss_total = 0.\n",
    "\n",
    "    # 時間の測定\n",
    "    synchronize()\n",
    "    start_time = time.time()\n",
    "\n",
    "    for x, t in dataloader:\n",
    "        # 順伝搬\n",
    "        y_pred = model(x)\n",
    "\n",
    "        # ロスの計算\n",
    "        loss = loss_fn(y_pred, t)\n",
    "        loss_total += loss.detach().numpy()\n",
    "\n",
    "        # 誤差逆伝播の前に各パラメータの勾配の値を0にセットする。\n",
    "        # これをしないと、勾配の値はそれまでの値との和がとられる。\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 誤差逆伝播。各パラメータの勾配が計算される。\n",
    "        loss.backward()\n",
    "\n",
    "        # 各パラメータの勾配の値を基に、optimizerにより値が更新される。\n",
    "        optimizer.step()\n",
    "\n",
    "    # ロス、accuracyをミニバッチの数で割って平均を取ります。\n",
    "    loss_total /= len(dataloader)\n",
    "    synchronize()  # GPUの処理が終わるのを待ちます。\n",
    "    print(f'epoch = {i_epoch}, time = {time.time() - start_time: .3f} sec, loss = {loss_total}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPUによるサンプルコード\n",
    "tensorやmodelをGPU上に移動する(`.cuda()`の追加)ことで、GPUを使ったトレーニングができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Sequential\n",
    "from torch.nn import Linear\n",
    "from torch.nn import Sigmoid\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import BCELoss\n",
    "from torch.optim import Adam\n",
    "from torch import from_numpy\n",
    "from torch.cuda import synchronize\n",
    "import time\n",
    "\n",
    "# モデルの定義\n",
    "model = Sequential(\n",
    "    Linear(in_features=100, out_features=256),  # ノード数が256の層を追加。\n",
    "    ReLU(),  # 活性化関数はReLU。\n",
    "    Linear(in_features=256, out_features=256),  # ノード数が256の層を追加。\n",
    "    ReLU(),  # 活性化関数はReLU。\n",
    "    Linear(in_features=256, out_features=256),  # ノード数が256の層を追加。\n",
    "    ReLU(),  # 活性化関数はReLU。\n",
    "    Linear(in_features=256, out_features=256),  # ノード数が256の層を追加。\n",
    "    ReLU(),  # 活性化関数はReLU。\n",
    "    Linear(in_features=256, out_features=256),  # ノード数が256の層を追加。\n",
    "    ReLU(),  # 活性化関数はReLU。\n",
    "    Linear(in_features=256, out_features=1),  # ノード数が1の層を追加。\n",
    "    Sigmoid(),  # 活性化関数はシグモイド関数。\n",
    ")\n",
    "# 誤差関数としてクロスエントロピーを指定。最適化手法は(確率的)勾配降下法\n",
    "loss_fn = BCELoss()\n",
    "optimizer = Adam(model.parameters())\n",
    "\n",
    "# GPUメモリにモデルをコピーします。\n",
    "model = model.cuda()\n",
    "\n",
    "# トレーニング\n",
    "for i_epoch in range(5):\n",
    "\n",
    "    # エポックごとのロス、accuracyを計算するための変数\n",
    "    loss_total = 0.\n",
    "\n",
    "    # 時間の測定\n",
    "    synchronize()\n",
    "    start_time = time.time()\n",
    "\n",
    "    for x, t in dataloader:\n",
    "        # GPUメモリにデータをコピーします。\n",
    "        x = x.cuda()\n",
    "        t = t.cuda()\n",
    "\n",
    "        # 順伝搬\n",
    "        y_pred = model(x)\n",
    "\n",
    "        # ロスの計算\n",
    "        loss = loss_fn(y_pred, t)\n",
    "        loss_total += loss.detach().cpu().numpy()  # lossの合計値を記録するため、値をCPUにコピーします。\n",
    "\n",
    "        # 誤差逆伝播の前に各パラメータの勾配の値を0にセットする。\n",
    "        # これをしないと、勾配の値はそれまでの値との和がとられる。\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 誤差逆伝播。各パラメータの勾配が計算される。\n",
    "        loss.backward()\n",
    "\n",
    "        # 各パラメータの勾配の値を基に、optimizerにより値が更新される。\n",
    "        optimizer.step()\n",
    "\n",
    "    # ロス、accuracyをミニバッチの数で割って平均を取ります。\n",
    "    loss_total /= len(dataloader)\n",
    "    synchronize()  # GPUの処理が終わるのを待ちます。\n",
    "    print(f'epoch = {i_epoch}, time = {time.time() - start_time: .3f} sec, loss = {loss_total}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "計算時間はどのように変化したでしょうか。\n",
    "モデルサイズが変化すると、計算時間はどのように変わるでしょうか？"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
