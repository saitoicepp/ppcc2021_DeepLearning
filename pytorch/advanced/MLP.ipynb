{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorchが使うCPUの数を制限します。(VMを使う場合)\n",
    "%env OMP_NUM_THREADS=1\n",
    "%env MKL_NUM_THREADS=1\n",
    "\n",
    "from torch import set_num_threads, set_num_interop_threads\n",
    "num_threads = 1\n",
    "set_num_threads(num_threads)\n",
    "set_num_interop_threads(num_threads)\n",
    "\n",
    "#ライブラリのインポート\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP モデルのPyTorchによる実装\n",
    "基礎編でnumpyを用いて実装したMLPモデル、誤差逆伝搬等をPyTorchで書いてみます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Tensor\n",
    "基礎編numpy版ではデータをnumpy array型にして扱っていました。\n",
    "numpy arrayはnumpyはもちろんmatplotlibなど非常に多くのPythonライブラリで使用できるのですが、残念ながらPyTorchモデルの入力には使用できません。\n",
    "PyTorchモデルへの入力はTorch Tensor型に変換する必要があります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch Tensorは`torch.tensor(3.14)`のようにして定義することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.tensor(3.14)\n",
    "b = torch.tensor(3)\n",
    "c = torch.tensor([1, 2, 3])\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy arrayからは、同様に`torch.tensor`を使用するか、`from_numpy`を使うことでtensor型に変換できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x_numpy = np.array([1, 2, 3])\n",
    "x_tensor_1 = torch.tensor(x_numpy)\n",
    "x_tensor_2 = torch.from_numpy(x_numpy)\n",
    "print(x_numpy)\n",
    "print(x_tensor_1)\n",
    "print(x_tensor_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "逆にtorch tensorからnumpy arrayに戻すときは`.numpy()`でできます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tensor_1.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensor型同士の演算はnumpyと同様に行うことができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([1, 2])\n",
    "b = torch.tensor([3, 4])\n",
    "print(\"a          = \", a)\n",
    "print(\"b          = \", b)\n",
    "print(\"2 * a      = \", 2 * a)\n",
    "print(\"a + b      = \", a + b)\n",
    "print(\"a - b      = \", a - b)\n",
    "print(\"a * b      = \", a * b)\n",
    "print(\"square(a)  = \", torch.square(a))\n",
    "print(\"sigmoid(a) = \", torch.sigmoid(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyTorch Tensor型と自動微分(autograd)\n",
    "\n",
    "PyTorch tensorは自動微分の機能が備わっています。この機能により、複雑な深層学習モデルの学習を容易に実行することが可能となります。\n",
    "\n",
    "例として、$y=x_1^2 + x_2^2$の$(x_1, x_2)=(4.0, 3.0)$での勾配を求めてみます。\n",
    "手で計算すると\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\left. \\frac{\\partial y}{\\partial x_1} \\right|_{x_1=4.0} = \\left. 2\\cdot x_1 \\right|_{x_1=4.0} = 8.0 \\\\\n",
    "\\left. \\frac{\\partial y}{\\partial x_2} \\right|_{x_2=3.0} = \\left. 2\\cdot x_2 \\right|_{x_2=3.0} = 6.0\n",
    "\\end{align*}\n",
    "$$\n",
    "となります。\n",
    "\n",
    "自動微分の機能を使用するには`requires_grad=True`を引数に指定してtensorを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = torch.tensor(4.0, requires_grad=True)\n",
    "x2 = torch.tensor(3.0, requires_grad=True)\n",
    "y = x1 ** 2 + x2 ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ただ値の演算が行われているだけに見えますが、計算グラフが構築されています。\n",
    "例えば以下のようにして計算グラフを可視化することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchviz import make_dot\n",
    "\n",
    "make_dot(y, params={\"x1\": x1, \"x2\": x2, \"y\": y})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.backward()`を実行すると、その変数(ここではy)の計算グラフから自動微分が実行され、計算グラフ内の各tensorに勾配の値が記録されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()\n",
    "print(\"x1の勾配: \", x1.grad)\n",
    "print(\"x2の勾配: \", x2.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "勾配情報が付加されたtensorを他のライブラリで使う際には注意が必要です。\n",
    "例えば`.numpy()`でnumpy arrayに変換しようとするとうまくいきません。\n",
    "```python\n",
    "x1.numpy()\n",
    ">>> RuntimeError: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.\n",
    "```\n",
    "\n",
    "`.detach()`で勾配情報を切り離す必要があります。\n",
    "```python\n",
    "x1.detach().numpy()\n",
    ">>> array(4., dtype=float32)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorchでは深層学習モデルのパラメータを勾配計算可能なtensorとして扱い、自動微分可能な形で演算を実装することで、深層学習モデルの効率的な学習を可能にしています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデル定義\n",
    "基礎編numpy版では\n",
    "```python\n",
    "# パーセプトロンのパラメータ\n",
    "w1 = np.random.randn(2, 3)  # 入力ノード数=2, 出力ノード数=3\n",
    "b1 = np.random.randn(3)  # 出力ノード数=3\n",
    "w2 = np.random.randn(3, 1)  # 入力ノード数=3, 出力ノード数=1\n",
    "b2 = np.random.randn(1)  # 出力ノード数=1\n",
    "\n",
    "# 順伝搬\n",
    "a1 = np.dot(x, w1) + b1\n",
    "z1 = sigmoid(a1)\n",
    "a2 = np.dot(z1, w2) + b2\n",
    "y = sigmoid(a2)\n",
    "```\n",
    "のようにしてMLPを実装していました。\n",
    "PyTorchでは\n",
    "```python\n",
    "import torch.nn as nn\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(in_features=2, out_features=3),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(in_features=3, out_features=1),\n",
    "    nn.Sigmoid(),\n",
    ")\n",
    "model(x)\n",
    "```\n",
    "のように書きます。\n",
    "\n",
    "<details>\n",
    "<summary>import torch.nn as nn について</summary>\n",
    "\n",
    "PyTorchのnnモジュールを`nn`という名前でimportしていますが、他にお好みで\n",
    "```python\n",
    "import torch\n",
    "torch.nn.Linear(in_features=2, out_features=3)\n",
    "```\n",
    "や\n",
    "```python\n",
    "from torch.nn import Linear\n",
    "Linear(in_features=2, out_features=3)\n",
    "```\n",
    "のようにも書けます。\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Linear` は行列計算($y=Wx+b$)をする関数です。\n",
    "活性化関数(ここでは`Sigmoid`)と組み合わせることで隠れ層を構成します。\n",
    "`Linear`の詳細は[公式のドキュメント](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)を参照することでわかります。\n",
    "ドキュメントを見ると、\n",
    "```python\n",
    "torch.nn.Linear(\n",
    "    in_features,\n",
    "    out_features,\n",
    "    bias=True,\n",
    "    dtype=None\n",
    ")\n",
    "```\n",
    "のような引数を持つことがわかります。また、各引数の意味は、\n",
    "* `in_features`:\tsize of each input sample.\n",
    "* `out_features`:\tsize of each output sample.\n",
    "* `bias`:\tIf set to False, the layer will not learn an additive bias. Default: True.\n",
    "\n",
    "のようになっています。隠れ層の入出力ノードの数、バイアス項の有無が指定できることがわかります。\n",
    "知らない関数を使うときは、必ずドキュメントを読んで、関数の入出力、引数、デフォルトの値などを確認するようにしましょう。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch Model (上の例では`model`)は`torchinfo`等の外部パッケージを使用することで、モデルの構成が確認できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchinfo\n",
    "\n",
    "import torch.nn as nn\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(in_features=2, out_features=3),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(in_features=3, out_features=1),\n",
    "    nn.Sigmoid(),\n",
    ")\n",
    "torchinfo.summary(model, input_size=(1, 2), col_names=[\"output_size\", \"num_params\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpyで定義したパラメータ数(w1:6個、b1:3個、w2:3個、b2:1個、計13個)と一致していることを確認してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習(誤差逆伝搬)\n",
    "\n",
    "基礎編numpy版では\n",
    "```python\n",
    "# 最急降下法での学習\n",
    "learning_rate = 1.0  # ステップ幅\n",
    "num_steps = 2000  # 繰り返し回数\n",
    "for _ in range(num_steps):\n",
    "    # 順伝搬させる\n",
    "    a1 = np.dot(x, w1) + b1\n",
    "    z1 = sigmoid(a1)\n",
    "    a2 = np.dot(z1, w2) + b2\n",
    "    y = sigmoid(a2)  # パーセプトロンの出力\n",
    "\n",
    "    # 一次微分を求めるために逆伝搬させる\n",
    "    grad_a2 = y - t\n",
    "    grad_w2 = np.einsum(\"ij,ik->ijk\", z1, grad_a2)  # grad_w2 = z1 * grad_a2\n",
    "    grad_b2 = 1.0 * grad_a2\n",
    "\n",
    "    grad_a1 = grad_sigmoid(a1) * (grad_a2 * w2.T)\n",
    "    grad_w1 = np.einsum(\"ij,ik->ijk\", x, grad_a1)  # grad_w1 = x * grad_a1\n",
    "    grad_b1 = 1.0 * grad_a1\n",
    "\n",
    "    # パラメータの更新 (mean(axis=0): 全てのデータ点に対しての平均値を使う)\n",
    "    w2 = w2 - learning_rate * grad_w2.mean(axis=0)\n",
    "    b2 = b2 - learning_rate * grad_b2.mean(axis=0)\n",
    "    w1 = w1 - learning_rate * grad_w1.mean(axis=0)\n",
    "    b1 = b1 - learning_rate * grad_b1.mean(axis=0)\n",
    "```\n",
    "のように勾配を計算し、パラメータのアップデートを行っていました。\n",
    "PyTorchでは\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 最急降下法での学習\n",
    "learning_rate = 1.0  # ステップ幅\n",
    "num_steps = 2000  # 繰り返し回数\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for i_epoch in range(num_steps):\n",
    "    # モデルをトレーニングモードにする。\n",
    "    model.train()\n",
    "\n",
    "    # 順伝搬\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # ロスの計算\n",
    "    loss = loss_fn(y_pred, t)\n",
    "\n",
    "    # 誤差逆伝播の前に各パラメータの勾配の値を0にセットする。\n",
    "    # これをしないと、勾配の値はそれまでの値との和がとられる。\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 誤差逆伝播。各パラメータの勾配が計算される。\n",
    "    loss.backward()\n",
    "\n",
    "    # 各パラメータの勾配の値を基に、optimizerにより値が更新される。\n",
    "    optimizer.step()\n",
    "```\n",
    "となります。`loss.backward()`を実行することにより、誤差逆伝搬が実行され、各モデルパラメータの誤差が計算されます。\n",
    "`optimizer.step()`では、optimizer(ここではSGD)の更新式に従ってパラメータの更新が行われます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1.0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### これまでのまとめ\n",
    "\n",
    "ここまでのコードをまとめて学習を実行してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# データ点の取得\n",
    "x, t = utils.dataset_for_mlp()\n",
    "\n",
    "# numpy -> torch.tensor\n",
    "x = torch.from_numpy(x).float()\n",
    "t = torch.from_numpy(t).float()\n",
    "\n",
    "# モデルの定義\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(in_features=2, out_features=3),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(in_features=3, out_features=1),\n",
    "    nn.Sigmoid(),\n",
    ")\n",
    "\n",
    "# 最急降下法での学習\n",
    "learning_rate = 1.0  # ステップ幅\n",
    "num_steps = 2000  # 繰り返し回数\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# トレーニング\n",
    "for i_epoch in range(num_steps):\n",
    "    # モデルをトレーニングモードにする。\n",
    "    model.train()\n",
    "\n",
    "    # 順伝搬\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # ロスの計算\n",
    "    loss = loss_fn(y_pred, t)\n",
    "\n",
    "    # 誤差逆伝播の前に各パラメータの勾配の値を0にセットする。\n",
    "    # これをしないと、勾配の値はそれまでの値との和がとられる。\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 誤差逆伝播。各パラメータの勾配が計算される。\n",
    "    loss.backward()\n",
    "\n",
    "    # 各パラメータの勾配の値を基に、optimizerにより値が更新される。\n",
    "    optimizer.step()\n",
    "\n",
    "# モデルを評価モードにする。\n",
    "model.eval()\n",
    "\n",
    "# Figureの作成 (キャンバスの作成)\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# パーセプトロンの出力を等高線プロット\n",
    "utils.plot_prediction(model, ax=ax)\n",
    "\n",
    "# データ点をプロット\n",
    "utils.plot_datapoint(x, t, ax=ax)\n",
    "\n",
    "# 図を表示\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch Model (上の例では`model`)は`torchinfo`等の外部パッケージを使用することで、モデルの構成が確認できます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデルの拡張"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "層の数を増やしてみましょう。新たな層を重ねることで層の数を増やすことができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(in_features=2, out_features=3),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(in_features=3, out_features=3),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(in_features=3, out_features=3),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(in_features=3, out_features=3),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(in_features=3, out_features=3),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(in_features=3, out_features=1),\n",
    "    nn.Sigmoid(),\n",
    ")\n",
    "\n",
    "torchinfo.summary(model, input_size=x.shape, col_names=[\"output_size\", \"num_params\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルのパラメータの数が増えていることがわかります。\n",
    "\n",
    "次に、ノードの数を増やしてみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(in_features=2, out_features=128),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(in_features=128, out_features=128),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(in_features=128, out_features=128),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(in_features=128, out_features=128),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(in_features=128, out_features=128),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(in_features=128, out_features=1),\n",
    "    nn.Sigmoid(),\n",
    ")\n",
    "\n",
    "torchinfo.summary(model, input_size=x.shape, col_names=[\"output_size\", \"num_params\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "パラメータの数が大きく増えたことがわかります。\n",
    "MLPにおいては、パラメータの数は、ノード数の2乗で増加します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このモデルを使って学習させてみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01  # ステップ幅\n",
    "num_steps = 3000  # 繰り返し回数\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# トレーニング\n",
    "for i_epoch in range(num_steps):\n",
    "    # モデルをトレーニングモードにする。\n",
    "    model.train()\n",
    "\n",
    "    # 順伝搬\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # ロスの計算\n",
    "    loss = loss_fn(y_pred, t)\n",
    "\n",
    "    # 誤差逆伝播の前に各パラメータの勾配の値を0にセットする。\n",
    "    # これをしないと、勾配の値はそれまでの値との和がとられる。\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 誤差逆伝播。各パラメータの勾配が計算される。\n",
    "    loss.backward()\n",
    "\n",
    "    # 各パラメータの勾配の値を基に、optimizerにより値が更新される。\n",
    "    optimizer.step()\n",
    "\n",
    "# モデルを評価モードにする。\n",
    "model.eval()\n",
    "\n",
    "# Figureの作成 (キャンバスの作成)\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# パーセプトロンの出力を等高線プロット\n",
    "utils.plot_prediction(model, ax=ax)\n",
    "\n",
    "# データ点をプロット\n",
    "utils.plot_datapoint(x, t, ax=ax)\n",
    "\n",
    "# 図を表示\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これまでは活性化関数としてシグモイド関数(`sigmoid`)を使っていました。昔はsigmoid関数やtanh関数がよく使われていましたが、最近はReLU関数がよく使われます。\n",
    "$$\n",
    "  ReLU = \\begin{cases}\n",
    "    x & (x \\geq 0) \\\\\n",
    "    0 & (x < 0)\n",
    "  \\end{cases}\n",
    "$$\n",
    "\n",
    "ReLUが好まれる理由については、別のnotebook(ActivationFunction.ipynb)を参照してください。\n",
    "\n",
    "ReLUを使って学習がどのようになるか確認してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# データ点の取得\n",
    "x, t = utils.dataset_for_mlp()\n",
    "\n",
    "# numpy -> torch.tensor\n",
    "x = torch.from_numpy(x).float()\n",
    "t = torch.from_numpy(t).float()\n",
    "\n",
    "# モデルの定義\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(in_features=2, out_features=128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=128, out_features=128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=128, out_features=128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=128, out_features=128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=128, out_features=128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=128, out_features=1),\n",
    "    nn.Sigmoid(),\n",
    ")\n",
    "\n",
    "learning_rate = 0.01  # ステップ幅\n",
    "num_steps = 3000  # 繰り返し回数\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# トレーニング\n",
    "for i_epoch in range(num_steps):\n",
    "    # モデルをトレーニングモードにする。\n",
    "    model.train()\n",
    "\n",
    "    # 順伝搬\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # ロスの計算\n",
    "    loss = loss_fn(y_pred, t)\n",
    "\n",
    "    # 誤差逆伝播の前に各パラメータの勾配の値を0にセットする。\n",
    "    # これをしないと、勾配の値はそれまでの値との和がとられる。\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 誤差逆伝播。各パラメータの勾配が計算される。\n",
    "    loss.backward()\n",
    "\n",
    "    # 各パラメータの勾配の値を基に、optimizerにより値が更新される。\n",
    "    optimizer.step()\n",
    "\n",
    "# モデルを評価モードにする。\n",
    "model.eval()\n",
    "\n",
    "# Figureの作成 (キャンバスの作成)\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# パーセプトロンの出力を等高線プロット\n",
    "utils.plot_prediction(model, ax=ax)\n",
    "\n",
    "# データ点をプロット\n",
    "utils.plot_datapoint(x, t, ax=ax)\n",
    "\n",
    "# 図を表示\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "深層学習をトレーニングするにあたって、最適化関数(optimizer)も非常に重要な要素です。\n",
    "確率的勾配降下法(SGD)の他によく使われるアルゴリズムとして adam があります。\n",
    "adamを使ってみると、どのようになるでしょうか。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# データ点の取得\n",
    "x, t = utils.dataset_for_mlp()\n",
    "\n",
    "# numpy -> torch.tensor\n",
    "x = torch.from_numpy(x).float()\n",
    "t = torch.from_numpy(t).float()\n",
    "\n",
    "# モデルの定義\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(in_features=2, out_features=128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=128, out_features=128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=128, out_features=128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=128, out_features=128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=128, out_features=128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=128, out_features=1),\n",
    "    nn.Sigmoid(),\n",
    ")\n",
    "\n",
    "num_steps = 3000  # 繰り返し回数\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# トレーニング\n",
    "for i_epoch in range(num_steps):\n",
    "    # モデルをトレーニングモードにする。\n",
    "    model.train()\n",
    "\n",
    "    # 順伝搬\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # ロスの計算\n",
    "    loss = loss_fn(y_pred, t)\n",
    "\n",
    "    # 誤差逆伝播の前に各パラメータの勾配の値を0にセットする。\n",
    "    # これをしないと、勾配の値はそれまでの値との和がとられる。\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 誤差逆伝播。各パラメータの勾配が計算される。\n",
    "    loss.backward()\n",
    "\n",
    "    # 各パラメータの勾配の値を基に、optimizerにより値が更新される。\n",
    "    optimizer.step()\n",
    "\n",
    "# モデルを評価モードにする。\n",
    "model.eval()\n",
    "\n",
    "# Figureの作成 (キャンバスの作成)\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# パーセプトロンの出力を等高線プロット\n",
    "utils.plot_prediction(model, ax=ax)\n",
    "\n",
    "# データ点をプロット\n",
    "utils.plot_datapoint(x, t, ax=ax)\n",
    "\n",
    "# 図を表示\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch モデルの定義方法\n",
    "PyTorchモデルを定義する方法はいくつかあります。\n",
    "最も簡単なのが`Sequential`を使った方法で、これまでの例では全てこの方法でモデルを定義してきました。\n",
    "一方で、少し複雑なモデルを考えると、`Sequential`モデルで対応できなくなってきます。\n",
    "一例としてResidual Network(ResNet)で使われるskip connectionを考えてみます。\n",
    "skip connectionは\n",
    "$$\n",
    "y = f_2(x + f_1(x))\n",
    "$$\n",
    "のように、入力を２つの経路に分け、片方はMLP、もう片方はそのまま後ろのレイヤーに接続します。\n",
    "このようなモデルは、途中入出力の分岐があるため、`Sequential`モデルでは実装できません。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorchモデルを定義する方法として、`Module`クラスのサブクラスを作る方法があります。\n",
    "`Module`クラスをカスタマイズすることができるので、特殊な学習をさせたいときなど、高度な深層学習モデルを扱うときに使われることもあります。\n",
    "`forward`という関数の中でモデル内のレイヤーの関係を定義します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelクラスを継承して新しいクラスを作成します\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class myModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear_1 = nn.Linear(in_features=2, out_features=128)\n",
    "        self.activation_1 = nn.ReLU()\n",
    "        self.linear_2 = nn.Linear(in_features=128, out_features=128)\n",
    "        self.activation_2 = nn.ReLU()\n",
    "        self.linear_3 = nn.Linear(in_features=128, out_features=128)\n",
    "        self.activation_3 = nn.ReLU()\n",
    "        self.linear_4 = nn.Linear(in_features=128, out_features=128)\n",
    "        self.activation_4 = nn.ReLU()\n",
    "        self.linear_5 = nn.Linear(in_features=128, out_features=1)\n",
    "        self.activation_5 = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.linear_1(inputs)\n",
    "        x = self.activation_1(x)\n",
    "        x = x + self.linear_2(x)  # skip connection\n",
    "        x = self.activation_2(x)\n",
    "        x = x + self.linear_3(x)  # skip connection\n",
    "        x = self.activation_3(x)\n",
    "        x = x + self.linear_4(x)  # skip connection\n",
    "        x = self.activation_4(x)\n",
    "        x = self.linear_5(x)\n",
    "        x = self.activation_5(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = myModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# データ点の取得\n",
    "x, t = utils.dataset_for_mlp()\n",
    "\n",
    "# numpy -> torch.tensor\n",
    "x = torch.from_numpy(x).float()\n",
    "t = torch.from_numpy(t).float()\n",
    "\n",
    "num_steps = 3000  # 繰り返し回数\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# トレーニング\n",
    "for i_epoch in range(num_steps):\n",
    "    # モデルをトレーニングモードにする。\n",
    "    model.train()\n",
    "\n",
    "    # 順伝搬\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # ロスの計算\n",
    "    loss = loss_fn(y_pred, t)\n",
    "\n",
    "    # 誤差逆伝播の前に各パラメータの勾配の値を0にセットする。\n",
    "    # これをしないと、勾配の値はそれまでの値との和がとられる。\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 誤差逆伝播。各パラメータの勾配が計算される。\n",
    "    loss.backward()\n",
    "\n",
    "    # 各パラメータの勾配の値を基に、optimizerにより値が更新される。\n",
    "    optimizer.step()\n",
    "\n",
    "# モデルを評価モードにする。\n",
    "model.eval()\n",
    "\n",
    "# Figureの作成 (キャンバスの作成)\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# パーセプトロンの出力を等高線プロット\n",
    "utils.plot_prediction(model, ax=ax)\n",
    "\n",
    "# データ点をプロット\n",
    "utils.plot_datapoint(x, t, ax=ax)\n",
    "\n",
    "# 図を表示\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 補足: tqdm\n",
    "PyTorchは学習中にその進捗を表示する機能を提供していません。\n",
    "長い学習中に現在の完了率がわからないと困るので、tqdmというライブラリが良く使われます。\n",
    "tqdmはiterativeオブジェクトラップするだけで機能します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "for _ in tqdm([1, 2, 3, 4, 5]):\n",
    "    time.sleep(1)  # 1秒スリープ\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習のためのforループにtqdmを使うことでプログレスバーを表示できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# トレーニング\n",
    "for i_epoch in tqdm(range(num_steps)):\n",
    "    # モデルをトレーニングモードにする。\n",
    "    model.train()\n",
    "\n",
    "    # 順伝搬\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # ロスの計算\n",
    "    loss = loss_fn(y_pred, t)\n",
    "\n",
    "    # 誤差逆伝播の前に各パラメータの勾配の値を0にセットする。\n",
    "    # これをしないと、勾配の値はそれまでの値との和がとられる。\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 誤差逆伝播。各パラメータの勾配が計算される。\n",
    "    loss.backward()\n",
    "\n",
    "    # 各パラメータの勾配の値を基に、optimizerにより値が更新される。\n",
    "    optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
