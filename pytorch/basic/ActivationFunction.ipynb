{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 勾配消失問題\n",
    "多層パーセプトロンでは、層の長さを長くすればするほど表現力は増します。一方で、学習が難しくなるという問題が知られています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorchが使うCPUの数を制限します。(VMを使う場合)\n",
    "%env OMP_NUM_THREADS=1\n",
    "%env MKL_NUM_THREADS=1\n",
    "\n",
    "from torch import set_num_threads, set_num_interop_threads\n",
    "num_threads = 1\n",
    "set_num_threads(num_threads)\n",
    "set_num_interop_threads(num_threads)\n",
    "\n",
    "#ライブラリのインポート\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# このセルはヘルパー関数です。\n",
    "\n",
    "# 上段: ウェイト(wij)の初期値をプロット\n",
    "def plot_weights(model):\n",
    "    iLayers = [0, 3, 6, 10]\n",
    "    labels = [\n",
    "        ' 0th layer',\n",
    "        ' 3th layer',\n",
    "        ' 6th layer',\n",
    "        'Last layer',\n",
    "    ]\n",
    "\n",
    "    values = [model.linears[i].weight.flatten().detach().numpy() for i in iLayers]\n",
    "    plt.hist(values, bins=50, stacked=False, density=True, label=labels, histtype='step')\n",
    "    plt.xlabel('weight')\n",
    "    plt.ylabel('Probability density')\n",
    "    plt.legend(loc='upper left', fontsize='x-small')\n",
    "    plt.show()\n",
    "\n",
    "# 中段: 各ノードの出力(sigma(ai))をプロット\n",
    "def plot_nodes(model, x):\n",
    "    iLayers = [0, 3, 6, 10]\n",
    "    labels = [\n",
    "        ' 0th layer',\n",
    "        ' 3th layer',\n",
    "        ' 6th layer',\n",
    "        'Last layer',\n",
    "    ]\n",
    "\n",
    "    values = [model(x, i).flatten().detach().numpy() for i in iLayers]\n",
    "    plt.hist(values, bins=50, stacked=False, density=True, label=labels, histtype='step')\n",
    "    plt.xlabel('activation')\n",
    "    plt.ylabel('Probability density')\n",
    "    plt.legend(loc='upper center', fontsize='x-small')\n",
    "    plt.show()\n",
    "\n",
    "# 下段: ウェイト(wij)の微分(dE/dwij)をプロット\n",
    "def plot_gradients(model):\n",
    "    iLayers = [0, 3, 6, 10]\n",
    "    labels = [\n",
    "        ' 0th layer',\n",
    "        ' 3th layer',\n",
    "        ' 6th layer',\n",
    "        'Last layer',\n",
    "    ]\n",
    "\n",
    "    grads = [np.abs(model.linears[i].weight.grad.flatten().detach().numpy()) for i in iLayers]\n",
    "    grads = [np.log10(x[x > 0]) for x in grads]\n",
    "    plt.hist(grads, bins=50, stacked=False, density=True, label=labels, histtype='step')\n",
    "    plt.xlabel('log10(|gradient of weights|)')\n",
    "    plt.ylabel('Probability density')\n",
    "    plt.legend(loc='upper left', fontsize='x-small')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "中間層が10層という深い多層パーセプトロンを用いて、モデル中の重みパラメータの大きさ、勾配の大きさを調べてみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Modelクラスを継承して新しいクラスを作成します\n",
    "from torch.nn import Module\n",
    "from torch.nn import Linear\n",
    "from torch.nn import Sigmoid\n",
    "\n",
    "class DeepMLP(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # 中間層が10層の多層パーセプトロン。各レイヤーのノード数は全て50。\n",
    "        self.linears = []\n",
    "        self.activations = []\n",
    "        for i in range(10):\n",
    "            self.linears += [Linear(in_features=50, out_features=50)]\n",
    "            self.activations += [Sigmoid()]\n",
    "        self.linears += [Linear(in_features=50, out_features=1)]\n",
    "        self.activations += [Sigmoid()]\n",
    "\n",
    "        # パラメータの初期化\n",
    "        from torch.nn import init\n",
    "        for layer in self.linears:\n",
    "            init.normal_(layer.weight, mean=0.0, std=1.0)  # weight(wij)の初期値。ここでは正規分布に従って初期化する\n",
    "            init.zeros_(layer.bias)  # bias termの初期値。ここでは0に初期化する。\n",
    "\n",
    "    def forward(self, inputs, last_node_index=-1):\n",
    "        x = inputs\n",
    "        for i, (linear, activation) in enumerate(zip(self.linears, self.activations)):\n",
    "            x = linear(x)\n",
    "            x = activation(x)\n",
    "            if last_node_index >= 0 and i >= last_node_index:\n",
    "                break\n",
    "        return x\n",
    "\n",
    "\n",
    "from torch.nn import BCELoss\n",
    "\n",
    "# データセットの生成\n",
    "nSamples = 1000\n",
    "nFeatures = 50\n",
    "x = np.random.randn(nSamples, nFeatures)  # 100個の入力変数を持つイベント1000個生成。それぞれの入力変数は正規分布に従う\n",
    "t = np.random.randint(2, size=nSamples).reshape([nSamples, 1])  # 正解ラベルは0 or 1でランダムに生成\n",
    "\n",
    "# numpy -> torch.tensor\n",
    "from torch import from_numpy\n",
    "x_tensor = from_numpy(x).float()\n",
    "t_tensor = from_numpy(t).float()\n",
    "\n",
    "# 中間層が10層の多層パーセプトロン。各レイヤーのノード数は全て50。\n",
    "model = DeepMLP()\n",
    "\n",
    "# 順伝搬・逆伝搬をして勾配を計算\n",
    "y_pred = model(x_tensor)\n",
    "loss = BCELoss()(y_pred, t_tensor)\n",
    "loss.backward()\n",
    "\n",
    "# ウェイト(wij)の初期値をプロット\n",
    "plot_weights(model)\n",
    "\n",
    "# 各ノードの出力(sigma(ai))をプロット\n",
    "plot_nodes(model, x_tensor)\n",
    "\n",
    "# ウェイト(wij)の微分(dE/dwij)をプロット\n",
    "plot_gradients(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上段のプロットはパラメータ($w_{ij}$)の初期値を表しています。指定したとおり、各層で正規分布に従って初期化されています。\n",
    "中段のプロットは活性化関数の出力($z_i$)を表しています。パラメータ($w_{ij}$)の初期値として正規分布を指定すると、シグモイド関数の出力はそのほとんどが0か1に非常に近い値となっています。シグモイド関数の微分は$\\sigma^{'}(x)=\\sigma(x)\\cdot(1-\\sigma(x))$なので、$\\sigma(x)$が0や1に近いときは微分値も非常に小さな値となります。\n",
    "誤差逆伝播の式は\n",
    "$$\n",
    "\\begin{align}\n",
    "\\delta_{i}^{(k)} &= \\sigma^{'}(a_i^{(k)}) \\left( \\sum_j w_{ij}^{(k+1)} \\cdot \\delta_{j}^{(k+1)} \\right) \\\\\n",
    "\\frac{\\partial E_n}{\\partial w_{ij}^{(k)}}  &= \\delta_{j}^{(k)} \\cdot z_{i}^{(k)}\n",
    "\\end{align}\n",
    "$$\n",
    "でした。$\\sigma^{'}(a_i^{(k)})$が小さいと後方の層から前方の層に誤差が伝わる際に、値が小さくなってしまいます。\n",
    "下段のプロットは各層での$\\frac{\\partial E_n}{\\partial w_{ij}^{(k)}}$を表しています。\n",
    "前方の層(0th layer)は後方の層と比較して分布の絶対値が小さくなっています。\n",
    "\n",
    "このように誤差が前の層にいくにつれて小さくなるため、前の層が後ろの層と比較して学習が進まなくなります。\n",
    "この問題は勾配消失の問題として知られています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "勾配消失はパラメータの初期値や、活性化関数を変更することによって解決・緩和することがわかっています。\n",
    "PyTorchの\n",
    "- [初期化のページ](https://pytorch.org/docs/stable/nn.init.html)\n",
    "- [活性化関数のページ](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity)\n",
    "\n",
    "も参考にしながら、この問題の解決を試みてみましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "活性化関数・パラメータの初期化方法の変更はモデル定義の中の\"activations\"、\"init\"の箇所を変更することによって行えます。\n",
    "\n",
    "例えばパラメータの初期化を(-0.01, +0.01)の一様分布に変更するときは以下の箇所のコード\n",
    "```python\n",
    "init.normal_(layer.weight, mean=0.0, std=1.0)  # weight(wij)の初期値。ここでは正規分布に従って初期化する\n",
    "```\n",
    "のようにすれば良いです。\n",
    "```python\n",
    "init.uniform_(layer.weight, a=-0.01, b=+0.01)  # weight(wij)の初期値。ここでは一様分布に従って初期化する\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelクラスを継承して新しいクラスを作成します\n",
    "from torch.nn import Module\n",
    "from torch.nn import Linear\n",
    "from torch.nn import Sigmoid\n",
    "\n",
    "class DeepMLP(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # 中間層が10層の多層パーセプトロン。各レイヤーのノード数は全て50。\n",
    "        self.linears = []\n",
    "        self.activations = []\n",
    "        for i in range(10):\n",
    "            self.linears += [Linear(in_features=50, out_features=50)]\n",
    "            self.activations += [Sigmoid()]\n",
    "        self.linears += [Linear(in_features=50, out_features=1)]\n",
    "        self.activations += [Sigmoid()]\n",
    "\n",
    "        # パラメータの初期化\n",
    "        from torch.nn import init\n",
    "        for layer in self.linears:\n",
    "            '''\n",
    "                CHANGED: 変更箇所 ここから\n",
    "            '''\n",
    "            init.uniform_(layer.weight, a=-0.01, b=+0.01)  # weight(wij)の初期値。ここでは一様分布に従って初期化する\n",
    "            '''\n",
    "                CHANGED: 変更箇所 ここまで\n",
    "            '''\n",
    "            init.zeros_(layer.bias)  # bias termの初期値。ここでは0に初期化する。\n",
    "\n",
    "    def forward(self, inputs, last_node_index=-1):\n",
    "        x = inputs\n",
    "        for i, (linear, activation) in enumerate(zip(self.linears, self.activations)):\n",
    "            x = linear(x)\n",
    "            x = activation(x)\n",
    "            if last_node_index >= 0 and i >= last_node_index:\n",
    "                break\n",
    "        return x\n",
    "\n",
    "from torch.nn import BCELoss\n",
    "\n",
    "# データセットの生成\n",
    "nSamples = 1000\n",
    "nFeatures = 50\n",
    "x = np.random.randn(nSamples, nFeatures)  # 100個の入力変数を持つイベント1000個生成。それぞれの入力変数は正規分布に従う\n",
    "t = np.random.randint(2, size=nSamples).reshape([nSamples, 1])  # 正解ラベルは0 or 1でランダムに生成\n",
    "\n",
    "# numpy -> torch.tensor\n",
    "from torch import from_numpy\n",
    "x_tensor = from_numpy(x).float()\n",
    "t_tensor = from_numpy(t).float()\n",
    "\n",
    "# 中間層が10層の多層パーセプトロン。各レイヤーのノード数は全て50。\n",
    "model = DeepMLP()\n",
    "\n",
    "# 順伝搬・逆伝搬をして勾配を計算\n",
    "y_pred = model(x_tensor)\n",
    "loss = BCELoss()(y_pred, t_tensor)\n",
    "loss.backward()\n",
    "\n",
    "# ウェイト(wij)の初期値をプロット\n",
    "plot_weights(model)\n",
    "\n",
    "# 各ノードの出力(sigma(ai))をプロット\n",
    "plot_nodes(model, x_tensor)\n",
    "\n",
    "# ウェイト(wij)の微分(dE/dwij)をプロット\n",
    "plot_gradients(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この例では活性化関数の出力が0.5付近に集中しています。\n",
    "どのノードも同じ出力をしているということはノード数を増やした意味があまりなくなっており、多層パーセプトロンの表現力が十分に活かしきれていないことがわかります。\n",
    "また、勾配消失も先程の例と比較して大きくなっています。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "f5ee84ca7cc839add57a1456079373a6ef1d5daac4f4be388eaa02049720b4e1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
